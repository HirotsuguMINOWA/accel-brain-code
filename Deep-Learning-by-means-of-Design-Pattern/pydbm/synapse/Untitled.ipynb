{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>RNN制限ボルツマンマシン</h4>\n",
    "\n",
    "TRBMやRTRBMは、時系列的なパターンを対象とした生成モデルとしては機能的に等価であるものの、その構造はまだ、再帰的ニューラルネットワークから区別されている。「RNN制限ボルツマンマシン(RNN-RBM)」は、とりわけRTRBMの構造を再帰的ニューラルネットワークに近付けて再記述されたモデルとして提唱されている。\n",
    "\n",
    "RTRBMにおいて、$$h'_{t-1}$$に依存するのはバイアスのみである。可視層と隠れ層のバイアスはそれぞれ次のようになる。\n",
    "\n",
    "$$b_V = b_V + W''h'_{t-1}$$\n",
    "\n",
    "$$b_H = b_H + W'h'_{t-1}$$\n",
    "\n",
    "隠れ層の各ユニット$$h_t$$がサンプリングから推論まで常にバイナリである一方で、$$h'_{t-1}$$は後続のユニットに伝播される確率場である。訓練精度を向上させると共に、$$h'_{t}$$の正確な推論を容易にする上で、この区別は重要となる。と言うのも$$h'_{t}$$は、結局のところ次のような特徴写像によって記述できるためである。\n",
    "\n",
    "$$h'_t = \\sigma (Wv_t + b_H) = \\sigma (Wv_t + W'h_{t-1} + b_H)$$\n",
    "\n",
    "RTRBMは、隠れ層のユニットが条件付き分布を構成することで、時間方向の情報を伝播しなければならないという制約を持つ。その確率モデルは、決定論的なRNNの出力である条件付き制限ボルツマンマシンの系列として理解することができる。しかしこの制約は、下図のように、別個の隠れ層のユニットを導入することで、解消することができる。このモデルを特に「RNN制限ボルツマンマシン(RNN-RBM)」と呼ぶ。\n",
    "\n",
    "この構成において、$$h'_{t}$$の特徴写像は次のように再記述される。\n",
    "\n",
    "$$h'_t = \\sigma (W_2v_t + W_3h'_{t-1} + b_{h'})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
