{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydbm.nn.simpleautoencoder.contractive_auto_encoder import ContractiveAutoEncoder\n",
    "from pydbm.nn.neural_network import NeuralNetwork as Encoder\n",
    "from pydbm.nn.neural_network import NeuralNetwork as Decoder\n",
    "from pydbm.nn.nn_layer import NNLayer as EncoderLayer\n",
    "from pydbm.nn.nn_layer import NNLayer as DecoderLayer\n",
    "from pydbm.activation.relu_function import ReLuFunction\n",
    "from pydbm.activation.tanh_function import TanhFunction\n",
    "from pydbm.activation.identity_function import IdentityFunction\n",
    "from pydbm.activation.logistic_function import LogisticFunction\n",
    "from pydbm.loss.mean_squared_error import MeanSquaredError\n",
    "from pydbm.optimization.optparams.adam import Adam\n",
    "from pydbm.synapse.nn_graph import NNGraph as EncoderGraph\n",
    "from pydbm.synapse.nn_graph import NNGraph as DecoderGraph\n",
    "from pydbm.verification.verificate_function_approximation import VerificateFunctionApproximation\n",
    "from pydbm.optimization.batch_norm import BatchNorm\n",
    "\n",
    "from logging import getLogger, StreamHandler, NullHandler, DEBUG, ERROR\n",
    "\n",
    "logger = getLogger(\"pydbm\")\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(DEBUG)\n",
    "logger.setLevel(DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 50\n",
    "dim1 = 100\n",
    "dim2 = 500\n",
    "scale = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_activation_function = LogisticFunction()\n",
    "#encoder_activation_function.batch_norm = BatchNorm()\n",
    "decoder_activation_function = IdentityFunction()\n",
    "#decoder_activation_function.batch_norm = BatchNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup NN layers and the parameters.\n"
     ]
    }
   ],
   "source": [
    "encoder_graph = EncoderGraph(\n",
    "    activation_function=encoder_activation_function,\n",
    "    hidden_neuron_count=dim1,\n",
    "    output_neuron_count=dim2,\n",
    "    scale=scale,\n",
    ")\n",
    "\n",
    "encoder_layer = EncoderLayer(encoder_graph)\n",
    "\n",
    "opt_params = Adam()\n",
    "opt_params.dropout_rate = 0.5\n",
    "\n",
    "encoder = Encoder(\n",
    "    nn_layer_list=[\n",
    "        encoder_layer, \n",
    "    ],\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=1e-01,\n",
    "    learning_attenuate_rate=0.1,\n",
    "    attenuate_epoch=50,\n",
    "    computable_loss=MeanSquaredError(grad_clip_threshold=1e+10),\n",
    "    opt_params=opt_params,\n",
    "    verificatable_result=VerificateFunctionApproximation(),\n",
    "    test_size_rate=0.3,\n",
    "    tol=1e-15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup NN layers and the parameters.\n"
     ]
    }
   ],
   "source": [
    "decoder_graph = DecoderGraph(\n",
    "    activation_function=decoder_activation_function,\n",
    "    hidden_neuron_count=dim2,\n",
    "    output_neuron_count=dim1,\n",
    "    scale=scale,\n",
    ")\n",
    "decoder_graph.tied_graph = encoder_graph\n",
    "\n",
    "decoder_layer = DecoderLayer(decoder_graph)\n",
    "\n",
    "opt_params = Adam()\n",
    "opt_params.dropout_rate = 0.5\n",
    "\n",
    "decoder = Decoder(\n",
    "    nn_layer_list=[\n",
    "        decoder_layer, \n",
    "    ],\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=1e-01,\n",
    "    learning_attenuate_rate=0.1,\n",
    "    attenuate_epoch=50,\n",
    "    computable_loss=MeanSquaredError(grad_clip_threshold=1e+10),\n",
    "    opt_params=opt_params,\n",
    "    verificatable_result=VerificateFunctionApproximation(),\n",
    "    test_size_rate=0.3,\n",
    "    tol=1e-15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup NN layers and the parameters.\n"
     ]
    }
   ],
   "source": [
    "auto_encoder = ContractiveAutoEncoder(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=1e-01,\n",
    "    learning_attenuate_rate=0.1,\n",
    "    attenuate_epoch=50,\n",
    "    computable_loss=MeanSquaredError(grad_clip_threshold=1e+10),\n",
    "    verificatable_result=VerificateFunctionApproximation(),\n",
    "    test_size_rate=0.3,\n",
    "    tol=1e-15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_arr = np.random.normal(loc=0.0, scale=1.0, size=(1000, dim1))\n",
    "observed_arr = (observed_arr - observed_arr.mean()) / (observed_arr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive hyperparameter that controls the strength of the regularization.\n",
    "auto_encoder.penalty_lambda = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NN starts learning.\n",
      "Best params are updated.\n",
      "Epoch: 1\n",
      "Loss: \n",
      "Training: 0.9928250506993256 Test: 2.087421439748838\n",
      "Epoch: 2\n",
      "Loss: \n",
      "Training: 2.775029790600879 Test: 2.78381293736667\n",
      "Epoch: 3\n",
      "Loss: \n",
      "Training: 4.865600618832811 Test: 4.054767169556278\n",
      "Epoch: 4\n",
      "Loss: \n",
      "Training: 8.864269806711786 Test: 4.316400102595504\n",
      "Epoch: 5\n",
      "Loss: \n",
      "Training: 8.07180476364713 Test: 3.6536370351378595\n",
      "Epoch: 6\n",
      "Loss: \n",
      "Training: 6.264423211085205 Test: 3.7155613166301076\n",
      "Epoch: 7\n",
      "Loss: \n",
      "Training: 5.839623304593101 Test: 4.010468196812489\n",
      "Epoch: 8\n",
      "Loss: \n",
      "Training: 5.1365198770188405 Test: 3.674682765496803\n",
      "Epoch: 9\n",
      "Loss: \n",
      "Training: 6.393472820060461 Test: 2.7656163753681366\n",
      "Epoch: 10\n",
      "Loss: \n",
      "Training: 5.9356314540670985 Test: 3.108507805130708\n",
      "Epoch: 11\n",
      "Loss: \n",
      "Training: 5.302546855394119 Test: 3.372690493265484\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 5.513920069731664 Test: 3.4170875143843396\n",
      "Epoch: 12\n",
      "Loss: \n",
      "Training: 6.369532568631907 Test: 3.509932059502874\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 5.944892250201144 Test: 3.5456144197360047\n",
      "Epoch: 13\n",
      "Loss: \n",
      "Training: 8.061855432575348 Test: 3.170525273853798\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 6.304342528004246 Test: 3.618226331949624\n",
      "Epoch: 14\n",
      "Loss: \n",
      "Training: 6.323794200488183 Test: 3.2962657295713536\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 6.6239680093784985 Test: 3.5298021423793764\n",
      "Epoch: 15\n",
      "Loss: \n",
      "Training: 5.294895627929796 Test: 2.760352716141013\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 6.369920448756139 Test: 3.4277887050769613\n",
      "Epoch: 16\n",
      "Loss: \n",
      "Training: 7.233424828878459 Test: 2.547294393550992\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 6.092229535184406 Test: 3.338460273177277\n",
      "Epoch: 17\n",
      "Loss: \n",
      "Training: 5.490118899562592 Test: 2.754961158238909\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 6.189129696963731 Test: 3.221633580869365\n",
      "Epoch: 18\n",
      "Loss: \n",
      "Training: 5.448056433092978 Test: 2.721760951826036\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 6.154179256460679 Test: 3.096082877012007\n",
      "Epoch: 19\n",
      "Loss: \n",
      "Training: 5.399329495393269 Test: 2.578550962908459\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 6.185332912068093 Test: 3.0007906956449304\n",
      "Epoch: 20\n",
      "Loss: \n",
      "Training: 5.339435526500931 Test: 2.391247585759977\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 6.085918579601374 Test: 2.9820841543989625\n",
      "Epoch: 21\n",
      "Loss: \n",
      "Training: 4.365087580412775 Test: 2.5376425544562338\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 6.026298986844757 Test: 2.9103581324618895\n",
      "Epoch: 22\n",
      "Loss: \n",
      "Training: 5.508064390763112 Test: 2.1369581877744768\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 5.932553059346623 Test: 2.8268533385809644\n",
      "Epoch: 23\n",
      "Loss: \n",
      "Training: 4.208019819129324 Test: 1.9835578337923263\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 5.846406241559744 Test: 2.689555951408125\n",
      "Epoch: 24\n",
      "Loss: \n",
      "Training: 5.13312833613136 Test: 2.1997630706755915\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 5.461022680215142 Test: 2.570859207401978\n",
      "Epoch: 25\n",
      "Loss: \n",
      "Training: 4.906141777672945 Test: 1.8283504333159766\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 5.34195609377946 Test: 2.4612089415124014\n",
      "Epoch: 26\n",
      "Loss: \n",
      "Training: 4.454331278129203 Test: 1.8507116464502074\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 5.3030807087537735 Test: 2.3680087132298975\n",
      "Epoch: 27\n",
      "Loss: \n",
      "Training: 4.733903134062286 Test: 1.6787286881933279\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 5.025171353678848 Test: 2.2983504385198192\n",
      "Epoch: 28\n",
      "Loss: \n",
      "Training: 3.783749710224662 Test: 1.6492079008287757\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.9495497771288175 Test: 2.1907271915152613\n",
      "Epoch: 29\n",
      "Loss: \n",
      "Training: 4.860832848661485 Test: 1.636376229472432\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.783119104841987 Test: 2.083471886415535\n",
      "Epoch: 30\n",
      "Loss: \n",
      "Training: 4.403338691677573 Test: 1.168941654107972\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.7292694401688085 Test: 1.989254413071933\n",
      "Epoch: 31\n",
      "Loss: \n",
      "Training: 3.420433013576783 Test: 1.5698811400765138\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.635659756686473 Test: 1.867023819906732\n",
      "Epoch: 32\n",
      "Loss: \n",
      "Training: 3.823121917344487 Test: 1.68800956045278\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.541194300002874 Test: 1.7702476784687602\n",
      "Epoch: 33\n",
      "Loss: \n",
      "Training: 4.603205037844527 Test: 1.2181787219885405\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.372700052661012 Test: 1.7253528157365903\n",
      "Epoch: 34\n",
      "Loss: \n",
      "Training: 3.8084298599495816 Test: 1.1426620518648212\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.412218574532531 Test: 1.6488149045562117\n",
      "Epoch: 35\n",
      "Loss: \n",
      "Training: 3.9956681229342705 Test: 1.2171273319594131\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.279748726914353 Test: 1.5431048026751346\n",
      "Epoch: 36\n",
      "Loss: \n",
      "Training: 3.1852250660280537 Test: 1.2128766340091555\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.1887013614404855 Test: 1.4819824925394784\n",
      "Epoch: 37\n",
      "Loss: \n",
      "Training: 3.6132258032898608 Test: 1.225893211880801\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 4.061790740230371 Test: 1.418198991295373\n",
      "Epoch: 38\n",
      "Loss: \n",
      "Training: 3.8598156392653955 Test: 1.0214614644977498\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.9497230071531284 Test: 1.3729154436641207\n",
      "Epoch: 39\n",
      "Loss: \n",
      "Training: 3.5921628515890136 Test: 0.9570187273626353\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.9573296000572014 Test: 1.3101408000310177\n",
      "Epoch: 40\n",
      "Loss: \n",
      "Training: 3.430341990060595 Test: 0.8610976012065116\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.8304626003499544 Test: 1.242205049820038\n",
      "Epoch: 41\n",
      "Loss: \n",
      "Training: 3.7981041385120813 Test: 0.9469881482459095\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.733162930188257 Test: 1.211420644529892\n",
      "Epoch: 42\n",
      "Loss: \n",
      "Training: 4.067449557077603 Test: 0.8820929135073362\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.770930042681786 Test: 1.1491313453468317\n",
      "Epoch: 43\n",
      "Loss: \n",
      "Training: 3.0781485583784947 Test: 0.892451780225879\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.7953628066550977 Test: 1.0685396806522873\n",
      "Epoch: 44\n",
      "Loss: \n",
      "Training: 3.6642184439858787 Test: 1.0129683822929576\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.6428571587084946 Test: 1.0359669864760213\n",
      "Epoch: 45\n",
      "Loss: \n",
      "Training: 3.5002270381970755 Test: 1.073162760114792\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.628436017112125 Test: 1.0229976195188348\n",
      "Epoch: 46\n",
      "Loss: \n",
      "Training: 3.4828636947921603 Test: 1.0478264485135536\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.578891908638405 Test: 1.0086011623343727\n",
      "Epoch: 47\n",
      "Loss: \n",
      "Training: 3.287914634979066 Test: 0.8764603735453038\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.6086557715148153 Test: 0.9920961437848126\n",
      "Epoch: 48\n",
      "Loss: \n",
      "Training: 3.1331548307850547 Test: 0.9712038959921859\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.576124654683736 Test: 0.9571528599512629\n",
      "Epoch: 49\n",
      "Loss: \n",
      "Training: 3.4584152025669495 Test: 0.8007728489182274\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.5034585738357022 Test: 0.9521271031007064\n",
      "Epoch: 50\n",
      "Loss: \n",
      "Training: 2.9350257395882626 Test: 0.8568730886144608\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.490083808933496 Test: 0.9365025152562655\n",
      "Epoch: 51\n",
      "Loss: \n",
      "Training: 3.749297352860508 Test: 0.7413324388014447\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.4405521838862634 Test: 0.9360800639970606\n",
      "Epoch: 52\n",
      "Loss: \n",
      "Training: 2.9368705296718733 Test: 0.6322162907908645\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.4356715053211055 Test: 0.9155144930526141\n",
      "Epoch: 53\n",
      "Loss: \n",
      "Training: 2.7708894010604177 Test: 0.5696446368815834\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.3226136025805326 Test: 0.8905268307809668\n",
      "Epoch: 54\n",
      "Loss: \n",
      "Training: 3.1292431815948616 Test: 0.48478428194711115\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.2918876868487246 Test: 0.8582461164465375\n",
      "Epoch: 55\n",
      "Loss: \n",
      "Training: 2.9134103095986172 Test: 0.4524190507672414\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.2383901606096233 Test: 0.8054277064119526\n",
      "Epoch: 56\n",
      "Loss: \n",
      "Training: 2.53254030500496 Test: 0.45300303885355186\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.179708487749777 Test: 0.7433533354771977\n",
      "Epoch: 57\n",
      "Loss: \n",
      "Training: 3.0721195538416413 Test: 0.39203726796083793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.0846761487710577 Test: 0.6838709945111976\n",
      "Epoch: 58\n",
      "Loss: \n",
      "Training: 2.721216470758968 Test: 0.38304494476632345\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.063096640657315 Test: 0.6354286839527509\n",
      "Epoch: 59\n",
      "Loss: \n",
      "Training: 2.1486726218451877 Test: 0.4797069255254091\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 3.021902804654706 Test: 0.5766127888301646\n",
      "Epoch: 60\n",
      "Loss: \n",
      "Training: 2.298079510141492 Test: 0.4760300967369838\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.8909285465825296 Test: 0.5445061964908829\n",
      "Epoch: 61\n",
      "Loss: \n",
      "Training: 2.663137770769945 Test: 0.45333066995819626\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.8272339236378525 Test: 0.5064218973031351\n",
      "Epoch: 62\n",
      "Loss: \n",
      "Training: 2.1340819702708984 Test: 0.4757265697539823\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.7186179654287965 Test: 0.4776217204188102\n",
      "Epoch: 63\n",
      "Loss: \n",
      "Training: 2.792568052003013 Test: 0.5300261342583733\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.6383391094886988 Test: 0.46197274831512203\n",
      "Epoch: 64\n",
      "Loss: \n",
      "Training: 2.7787798095955063 Test: 0.49476178615912697\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.640506974582958 Test: 0.4580108980528011\n",
      "Epoch: 65\n",
      "Loss: \n",
      "Training: 2.3629860058747165 Test: 0.4453249871305561\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.6054606373830227 Test: 0.45900864847400263\n",
      "Epoch: 66\n",
      "Loss: \n",
      "Training: 2.9622990707451167 Test: 0.4242613080870186\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.5504182070106323 Test: 0.4582992421103341\n",
      "Epoch: 67\n",
      "Loss: \n",
      "Training: 2.615379543149083 Test: 0.4614372063311739\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.593394083584649 Test: 0.4554250690336808\n",
      "Epoch: 68\n",
      "Loss: \n",
      "Training: 2.4222304797098757 Test: 0.35029152489716736\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.5477200825153927 Test: 0.4623650628707144\n",
      "Epoch: 69\n",
      "Loss: \n",
      "Training: 2.3477328928765013 Test: 0.3645687719666769\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.5178214834104833 Test: 0.4590897208837988\n",
      "Epoch: 70\n",
      "Loss: \n",
      "Training: 2.852021115216135 Test: 0.3921341333054367\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.537727510513615 Test: 0.4475759055279255\n",
      "Epoch: 71\n",
      "Loss: \n",
      "Training: 2.292695302890088 Test: 0.3710855743245288\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.593121671021079 Test: 0.43918630918477086\n",
      "Epoch: 72\n",
      "Loss: \n",
      "Training: 2.421581629676848 Test: 0.39467942376991777\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.5560774242330933 Test: 0.43096179962140413\n",
      "Epoch: 73\n",
      "Loss: \n",
      "Training: 2.587687952983955 Test: 0.39853812133248356\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.5848273901736887 Test: 0.4228570850229977\n",
      "Epoch: 74\n",
      "Loss: \n",
      "Training: 2.3457601907406582 Test: 0.3534153953861119\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.564339380271783 Test: 0.4097082837304087\n",
      "Epoch: 75\n",
      "Loss: \n",
      "Training: 2.366188102725526 Test: 0.3797948788236483\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.521037418386298 Test: 0.3955736446531072\n",
      "Epoch: 76\n",
      "Loss: \n",
      "Training: 2.219634293338206 Test: 0.35767127966316264\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.521357628071379 Test: 0.3890206338224164\n",
      "Epoch: 77\n",
      "Loss: \n",
      "Training: 2.069050306472041 Test: 0.3748243311627734\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.447091150330688 Test: 0.3823616309800308\n",
      "Epoch: 78\n",
      "Loss: \n",
      "Training: 2.1017859957124543 Test: 0.3578354157254802\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.3924582266629835 Test: 0.37370034346319075\n",
      "Epoch: 79\n",
      "Loss: \n",
      "Training: 2.3016992975577444 Test: 0.338272503397312\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.3604137782632413 Test: 0.3744547325460221\n",
      "Epoch: 80\n",
      "Loss: \n",
      "Training: 2.062938780114243 Test: 0.34701446731638314\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.355810418731365 Test: 0.3718251056890856\n",
      "Epoch: 81\n",
      "Loss: \n",
      "Training: 2.195043541895335 Test: 0.3438596371268049\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.2769021852211764 Test: 0.36731313909018015\n",
      "Epoch: 82\n",
      "Loss: \n",
      "Training: 2.0997307377272336 Test: 0.38472158650704297\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.267137009121701 Test: 0.3645905453704078\n",
      "Epoch: 83\n",
      "Loss: \n",
      "Training: 2.390383859711065 Test: 0.3343307999445105\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.23495191992674 Test: 0.3635947616441203\n",
      "Epoch: 84\n",
      "Loss: \n",
      "Training: 2.1279217500096137 Test: 0.30876728153298816\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.215221510599451 Test: 0.357174029505323\n",
      "Epoch: 85\n",
      "Loss: \n",
      "Training: 2.241733042260934 Test: 0.3405766476196076\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.193437666526346 Test: 0.3527092181200106\n",
      "Epoch: 86\n",
      "Loss: \n",
      "Training: 2.057521825578168 Test: 0.40197324479129487\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.1809921604798865 Test: 0.34878739499960654\n",
      "Epoch: 87\n",
      "Loss: \n",
      "Training: 2.238930435568351 Test: 0.37299454913484353\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.164780913703883 Test: 0.3532175915124197\n",
      "Epoch: 88\n",
      "Loss: \n",
      "Training: 2.1407139884335837 Test: 0.3361974007145998\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.181768926613514 Test: 0.3530346133096268\n",
      "Epoch: 89\n",
      "Loss: \n",
      "Training: 2.278176251791061 Test: 0.36425288291788965\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.185661725885627 Test: 0.35087081180853874\n",
      "Epoch: 90\n",
      "Loss: \n",
      "Training: 1.9437625307503148 Test: 0.36441713457888547\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.1833094213089588 Test: 0.35346884976059656\n",
      "Epoch: 91\n",
      "Loss: \n",
      "Training: 2.0457274640988996 Test: 0.33958086618167177\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.171391796372566 Test: 0.3552091164868467\n",
      "Epoch: 92\n",
      "Loss: \n",
      "Training: 1.8612653982948657 Test: 0.3834823500606839\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.1564601885929227 Test: 0.3547812393923334\n",
      "Epoch: 93\n",
      "Loss: \n",
      "Training: 1.8706372733775614 Test: 0.343464562166622\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.1326136546496857 Test: 0.35465731574769754\n",
      "Epoch: 94\n",
      "Loss: \n",
      "Training: 1.6713282750182956 Test: 0.32882777615598596\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.080638996016335 Test: 0.3555706919699087\n",
      "Epoch: 95\n",
      "Loss: \n",
      "Training: 2.2320733609068846 Test: 0.3502316171672262\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.0349796485172034 Test: 0.3575767414322085\n",
      "Epoch: 96\n",
      "Loss: \n",
      "Training: 1.9176097665423073 Test: 0.3223722984195031\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.0340136803817987 Test: 0.35854223838697036\n",
      "Epoch: 97\n",
      "Loss: \n",
      "Training: 2.2417855464192704 Test: 0.415783817674146\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.0200224744782123 Test: 0.3505821437497912\n",
      "Epoch: 98\n",
      "Loss: \n",
      "Training: 1.5358959611865945 Test: 0.3350424374643916\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 2.0203079855633037 Test: 0.35486107060372135\n",
      "Epoch: 99\n",
      "Loss: \n",
      "Training: 1.6684768305859548 Test: 0.33591076561535493\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.959826182838605 Test: 0.35474557427870057\n",
      "Epoch: 100\n",
      "Loss: \n",
      "Training: 2.140939752561119 Test: 0.3778069557343441\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8988562407180951 Test: 0.351911362548447\n",
      "Epoch: 101\n",
      "Loss: \n",
      "Training: 1.8992790726344806 Test: 0.3157298164239379\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9185739628991754 Test: 0.3532503446639929\n",
      "Epoch: 102\n",
      "Loss: \n",
      "Training: 1.8583172699177937 Test: 0.38471014426964834\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9039291237527336 Test: 0.3508652396882196\n",
      "Epoch: 103\n",
      "Loss: \n",
      "Training: 2.011131463307143 Test: 0.33780940003338167\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9036343109150262 Test: 0.35098801910911603\n",
      "Epoch: 104\n",
      "Loss: \n",
      "Training: 1.8037342866809893 Test: 0.3844825674976558\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9176837299079845 Test: 0.350422502895792\n",
      "Epoch: 105\n",
      "Loss: \n",
      "Training: 1.797505416047127 Test: 0.35855981604737813\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9309243310742539 Test: 0.35598798202995896\n",
      "Epoch: 106\n",
      "Loss: \n",
      "Training: 1.7571011622335693 Test: 0.3847956670892813\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.887467536588278 Test: 0.35682080191797416\n",
      "Epoch: 107\n",
      "Loss: \n",
      "Training: 2.105983550018693 Test: 0.38276662438646697\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8714166761574043 Test: 0.363063138784952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 108\n",
      "Loss: \n",
      "Training: 2.151091633606795 Test: 0.33149819112199613\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.857836476517346 Test: 0.35976141945618406\n",
      "Epoch: 109\n",
      "Loss: \n",
      "Training: 2.006239257997854 Test: 0.3333599442829251\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9193560437593664 Test: 0.35940699482194455\n",
      "Epoch: 110\n",
      "Loss: \n",
      "Training: 1.899429529508001 Test: 0.33574959765447226\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9531322865005567 Test: 0.35915191268870156\n",
      "Epoch: 111\n",
      "Loss: \n",
      "Training: 1.9360787668888304 Test: 0.33892457687415684\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9289812641952444 Test: 0.3549461768807144\n",
      "Epoch: 112\n",
      "Loss: \n",
      "Training: 2.047207548188007 Test: 0.3207618094287453\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9326612336206797 Test: 0.3572656529257363\n",
      "Epoch: 113\n",
      "Loss: \n",
      "Training: 1.7364361767640337 Test: 0.29357229937041995\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9515502614477007 Test: 0.350870819441646\n",
      "Epoch: 114\n",
      "Loss: \n",
      "Training: 1.8105310392516265 Test: 0.3807765052743149\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9240807327933898 Test: 0.3464471093753498\n",
      "Epoch: 115\n",
      "Loss: \n",
      "Training: 1.6891008853262848 Test: 0.34249432099785176\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9247604080504535 Test: 0.34607650315301575\n",
      "Epoch: 116\n",
      "Loss: \n",
      "Training: 1.8070084056455966 Test: 0.3243432580925519\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9139199549783694 Test: 0.34446995364806304\n",
      "Epoch: 117\n",
      "Loss: \n",
      "Training: 1.8609221605457307 Test: 0.3716623798191213\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9189106793195723 Test: 0.33842471274839014\n",
      "Epoch: 118\n",
      "Loss: \n",
      "Training: 1.8582962013265552 Test: 0.37906576504690465\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.894404540372276 Test: 0.33731428829165555\n",
      "Epoch: 119\n",
      "Loss: \n",
      "Training: 1.702941607977845 Test: 0.31543811520154713\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.865124997144252 Test: 0.3420710456841464\n",
      "Epoch: 120\n",
      "Loss: \n",
      "Training: 1.6217997425660768 Test: 0.376662578236827\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8347952321422512 Test: 0.3402788627760086\n",
      "Epoch: 121\n",
      "Loss: \n",
      "Training: 1.8530616350585216 Test: 0.3032232401701452\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8070322534480585 Test: 0.3443701608342441\n",
      "Epoch: 122\n",
      "Loss: \n",
      "Training: 1.6694633842461815 Test: 0.3380316257237628\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7987305402650278 Test: 0.3408000271638429\n",
      "Epoch: 123\n",
      "Loss: \n",
      "Training: 1.978714792932914 Test: 0.31928223785126447\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.760956123870845 Test: 0.34252700879334463\n",
      "Epoch: 124\n",
      "Loss: \n",
      "Training: 2.166168172547488 Test: 0.3541758717689483\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7851839854877334 Test: 0.3450980026414291\n",
      "Epoch: 125\n",
      "Loss: \n",
      "Training: 2.0111065761722577 Test: 0.3765213711484738\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.820747698817319 Test: 0.3424379392908924\n",
      "Epoch: 126\n",
      "Loss: \n",
      "Training: 1.9419182482106212 Test: 0.33592715808044876\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8529482679019165 Test: 0.3458406443059546\n",
      "Epoch: 127\n",
      "Loss: \n",
      "Training: 1.8462348705378273 Test: 0.36671366628706537\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.866439252158419 Test: 0.34699903430474427\n",
      "Epoch: 128\n",
      "Loss: \n",
      "Training: 1.7797318094451955 Test: 0.3299618298745163\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8649705231576292 Test: 0.3465041629515387\n",
      "Epoch: 129\n",
      "Loss: \n",
      "Training: 1.737653297531089 Test: 0.2942566820479921\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.857114083969493 Test: 0.34159376943429987\n",
      "Epoch: 130\n",
      "Loss: \n",
      "Training: 1.9460145885450466 Test: 0.36201621715029153\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8605852529248175 Test: 0.3394756261189444\n",
      "Epoch: 131\n",
      "Loss: \n",
      "Training: 2.084663156741899 Test: 0.30095791357407964\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8930067375227142 Test: 0.3380109900102909\n",
      "Epoch: 132\n",
      "Loss: \n",
      "Training: 2.184819999591117 Test: 0.3322767042288594\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9161668896910524 Test: 0.33778445735068435\n",
      "Epoch: 133\n",
      "Loss: \n",
      "Training: 1.684588152540653 Test: 0.310717658458414\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9677025512255457 Test: 0.33720896520119403\n",
      "Epoch: 134\n",
      "Loss: \n",
      "Training: 1.5245107013379497 Test: 0.33876703178376283\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9382898871863194 Test: 0.3363525072619089\n",
      "Epoch: 135\n",
      "Loss: \n",
      "Training: 1.7681757425575837 Test: 0.34814655471017686\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8741241400653652 Test: 0.33481162326339037\n",
      "Epoch: 136\n",
      "Loss: \n",
      "Training: 2.042622960213326 Test: 0.3178284680978105\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8498310567038985 Test: 0.3319741416195607\n",
      "Epoch: 137\n",
      "Loss: \n",
      "Training: 2.063998211109633 Test: 0.30530619225004946\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8599015279041686 Test: 0.3301642726212969\n",
      "Epoch: 138\n",
      "Loss: \n",
      "Training: 2.0924602985888714 Test: 0.30596250183013723\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8816778619613495 Test: 0.32402352521759525\n",
      "Epoch: 139\n",
      "Loss: \n",
      "Training: 1.883048098350519 Test: 0.3749051759687011\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.912950710875717 Test: 0.32162359241315736\n",
      "Epoch: 140\n",
      "Loss: \n",
      "Training: 1.8478645868656822 Test: 0.3778733463799503\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9274901909576596 Test: 0.32968844180522827\n",
      "Epoch: 141\n",
      "Loss: \n",
      "Training: 1.9748889704695565 Test: 0.28302627412555614\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9176751907897234 Test: 0.33127415472819405\n",
      "Epoch: 142\n",
      "Loss: \n",
      "Training: 1.7499474342687933 Test: 0.3064502648808727\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9066977721624891 Test: 0.32948099078334175\n",
      "Epoch: 143\n",
      "Loss: \n",
      "Training: 1.838428836215294 Test: 0.32404892482271774\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8632105156302567 Test: 0.3268983468485431\n",
      "Epoch: 144\n",
      "Loss: \n",
      "Training: 1.9507171267143149 Test: 0.3139523365719191\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.878594583997721 Test: 0.3282314734849735\n",
      "Epoch: 145\n",
      "Loss: \n",
      "Training: 1.8085983157758536 Test: 0.29815680746711226\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9212152265353577 Test: 0.3257500039637891\n",
      "Epoch: 146\n",
      "Loss: \n",
      "Training: 2.235604290621807 Test: 0.3201155608122095\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9252574838571845 Test: 0.3207510292394827\n",
      "Epoch: 147\n",
      "Loss: \n",
      "Training: 1.9558370181209466 Test: 0.28381450466325825\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.944555616898033 Test: 0.3209797385109226\n",
      "Epoch: 148\n",
      "Loss: \n",
      "Training: 1.7427396754626254 Test: 0.3231441496629496\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9337394975991635 Test: 0.31883056975224344\n",
      "Epoch: 149\n",
      "Loss: \n",
      "Training: 1.7219818417047046 Test: 0.3580973150596193\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8987674352865391 Test: 0.32054873453552474\n",
      "Epoch: 150\n",
      "Loss: \n",
      "Training: 2.041808906143585 Test: 0.3329678981636606\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8826608096219577 Test: 0.3188679484446165\n",
      "Epoch: 151\n",
      "Loss: \n",
      "Training: 1.910594683836503 Test: 0.3011280903556095\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.902055241549748 Test: 0.31437740362298755\n",
      "Epoch: 152\n",
      "Loss: \n",
      "Training: 1.5999215839702887 Test: 0.3413797708938414\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.895625812886443 Test: 0.3161875852459929\n",
      "Epoch: 153\n",
      "Loss: \n",
      "Training: 1.9075149039692614 Test: 0.35951583825916666\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8806232278565922 Test: 0.3196805358472897\n",
      "Epoch: 154\n",
      "Loss: \n",
      "Training: 1.802187778940699 Test: 0.2863333766677509\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.887531834631989 Test: 0.3232272271909346\n",
      "Epoch: 155\n",
      "Loss: \n",
      "Training: 2.0845632208874476 Test: 0.332367563491268\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8726788998546275 Test: 0.32046533120051773\n",
      "Epoch: 156\n",
      "Loss: \n",
      "Training: 1.7861961964814363 Test: 0.32448793187049324\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.900275390365787 Test: 0.32388640680293335\n",
      "Epoch: 157\n",
      "Loss: \n",
      "Training: 1.7353120738880288 Test: 0.29730806779766533\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8553345809517499 Test: 0.32432364390876167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 158\n",
      "Loss: \n",
      "Training: 1.7354854219427798 Test: 0.33106114036246015\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8332820865284583 Test: 0.3256730002222024\n",
      "Epoch: 159\n",
      "Loss: \n",
      "Training: 1.7927173483157137 Test: 0.3555549207056282\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8325566611764734 Test: 0.3264646992921535\n",
      "Epoch: 160\n",
      "Loss: \n",
      "Training: 1.7350093416072245 Test: 0.29566277557505566\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8396302118375742 Test: 0.32621045985675445\n",
      "Epoch: 161\n",
      "Loss: \n",
      "Training: 1.8837150440371253 Test: 0.3327274390342849\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.808950255383938 Test: 0.32247994759789395\n",
      "Epoch: 162\n",
      "Loss: \n",
      "Training: 1.6283512826822015 Test: 0.32687020206711953\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8062622914040005 Test: 0.3256398824657615\n",
      "Epoch: 163\n",
      "Loss: \n",
      "Training: 1.8535025458653072 Test: 0.37644160640877006\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8091052612751917 Test: 0.32418892558308926\n",
      "Epoch: 164\n",
      "Loss: \n",
      "Training: 1.7911420420800264 Test: 0.3453254164951674\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8037040254647962 Test: 0.32588150239804964\n",
      "Epoch: 165\n",
      "Loss: \n",
      "Training: 1.904853636594896 Test: 0.32507856667688056\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.802599451778729 Test: 0.33178070638079127\n",
      "Epoch: 166\n",
      "Loss: \n",
      "Training: 1.6833516333910084 Test: 0.3113430457362585\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.784628493349474 Test: 0.3310518066993525\n",
      "Epoch: 167\n",
      "Loss: \n",
      "Training: 1.810397494502219 Test: 0.30981130141296537\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.774344037040431 Test: 0.32973731808592904\n",
      "Epoch: 168\n",
      "Loss: \n",
      "Training: 1.8964251523497027 Test: 0.3100758293562997\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7818525791018502 Test: 0.33098764144745907\n",
      "Epoch: 169\n",
      "Loss: \n",
      "Training: 1.8512726501191041 Test: 0.32834328586697537\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7979465521425424 Test: 0.328889110346843\n",
      "Epoch: 170\n",
      "Loss: \n",
      "Training: 2.1306860666397616 Test: 0.3065496778133861\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8038020823228815 Test: 0.32616794686297773\n",
      "Epoch: 171\n",
      "Loss: \n",
      "Training: 1.8303321104542964 Test: 0.3036387179365454\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8433697548261354 Test: 0.32725663708681074\n",
      "Epoch: 172\n",
      "Loss: \n",
      "Training: 1.7957825129448661 Test: 0.3075405829891042\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8380314614678526 Test: 0.32434776497703677\n",
      "Epoch: 173\n",
      "Loss: \n",
      "Training: 1.9443027900670404 Test: 0.3022732265530908\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.854774584494119 Test: 0.32241480306923526\n",
      "Epoch: 174\n",
      "Loss: \n",
      "Training: 1.8455504301566412 Test: 0.28750457237446697\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.863854608914292 Test: 0.31499796508366734\n",
      "Epoch: 175\n",
      "Loss: \n",
      "Training: 1.715791554230079 Test: 0.2972676896361345\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8692954477219537 Test: 0.30921588067159733\n",
      "Epoch: 176\n",
      "Loss: \n",
      "Training: 1.9799760295705398 Test: 0.3181637181831088\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.850389239485472 Test: 0.30643479296752274\n",
      "Epoch: 177\n",
      "Loss: \n",
      "Training: 1.9137382922674608 Test: 0.30487398423597467\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8800516791034252 Test: 0.30711686021220774\n",
      "Epoch: 178\n",
      "Loss: \n",
      "Training: 1.970448756387511 Test: 0.3074813020066674\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8903857588799493 Test: 0.3066231284945086\n",
      "Epoch: 179\n",
      "Loss: \n",
      "Training: 1.512163905389098 Test: 0.3694032842149089\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8977881192837303 Test: 0.30636367575954543\n",
      "Epoch: 180\n",
      "Loss: \n",
      "Training: 1.9249207322140145 Test: 0.3339787756841727\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8638772448107297 Test: 0.31046967559433875\n",
      "Epoch: 181\n",
      "Loss: \n",
      "Training: 1.8391105236237941 Test: 0.3061954864004147\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8433007113681552 Test: 0.31321258538141744\n",
      "Epoch: 182\n",
      "Loss: \n",
      "Training: 2.0113352803181312 Test: 0.29982853591232544\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8441785526851047 Test: 0.31346826222780433\n",
      "Epoch: 183\n",
      "Loss: \n",
      "Training: 1.973691973612036 Test: 0.317897904787745\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.865733829422431 Test: 0.3126970575201265\n",
      "Epoch: 184\n",
      "Loss: \n",
      "Training: 1.815414671833825 Test: 0.32383710576859553\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8686727477769303 Test: 0.3142595253435919\n",
      "Epoch: 185\n",
      "Loss: \n",
      "Training: 2.1316700349657784 Test: 0.3412877363607324\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8656591719446485 Test: 0.31789277868300475\n",
      "Epoch: 186\n",
      "Loss: \n",
      "Training: 1.709936282721327 Test: 0.29783407366241044\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9072470200182192 Test: 0.3222947833554645\n",
      "Epoch: 187\n",
      "Loss: \n",
      "Training: 1.486474865906093 Test: 0.30569682037080675\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8802430453332974 Test: 0.32026181890339467\n",
      "Epoch: 188\n",
      "Loss: \n",
      "Training: 1.839002946580015 Test: 0.31731850568684467\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8375167026971606 Test: 0.3203441025168779\n",
      "Epoch: 189\n",
      "Loss: \n",
      "Training: 1.5790492082851235 Test: 0.3204583442982127\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8243721217164115 Test: 0.32132782288489564\n",
      "Epoch: 190\n",
      "Loss: \n",
      "Training: 2.1234882545775062 Test: 0.3021422646735833\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8310606520060138 Test: 0.316433328893226\n",
      "Epoch: 191\n",
      "Loss: \n",
      "Training: 1.882242969685699 Test: 0.358589998083426\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8509174042423626 Test: 0.31324967779216706\n",
      "Epoch: 192\n",
      "Loss: \n",
      "Training: 1.89497303691052 Test: 0.3313290359646345\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8552306488485535 Test: 0.3184891289604682\n",
      "Epoch: 193\n",
      "Loss: \n",
      "Training: 1.62326187470288 Test: 0.30750355785950834\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8435944245077927 Test: 0.32163917896569916\n",
      "Epoch: 194\n",
      "Loss: \n",
      "Training: 1.8747459836689822 Test: 0.3871784454692404\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8085514146168766 Test: 0.32059974427287546\n",
      "Epoch: 195\n",
      "Loss: \n",
      "Training: 1.8776058642166376 Test: 0.33513206138006163\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8144845458003924 Test: 0.32693387824293996\n",
      "Epoch: 196\n",
      "Loss: \n",
      "Training: 2.04682295429668 Test: 0.3611600600840346\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7890781287254782 Test: 0.3263183107448729\n",
      "Epoch: 197\n",
      "Loss: \n",
      "Training: 1.679330258775932 Test: 0.3831669248037323\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8227667958830136 Test: 0.3326509093870353\n",
      "Epoch: 198\n",
      "Loss: \n",
      "Training: 1.83626282778693 Test: 0.33529923608213996\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8420523351699973 Test: 0.34039791983032786\n",
      "Epoch: 199\n",
      "Loss: \n",
      "Training: 1.8728514126482059 Test: 0.29280920428349244\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8417783232906886 Test: 0.34219599286985736\n",
      "Epoch: 200\n",
      "Loss: \n",
      "Training: 1.6561799038104335 Test: 0.2908416383563109\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.871158543726997 Test: 0.33943107886838536\n",
      "Epoch: 201\n",
      "Loss: \n",
      "Training: 1.9920249400727699 Test: 0.30306609703326903\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8244277086502898 Test: 0.3383010162366581\n",
      "Epoch: 202\n",
      "Loss: \n",
      "Training: 1.8475253129447082 Test: 0.3569358372694185\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8354059056889973 Test: 0.3327486261316424\n",
      "Epoch: 203\n",
      "Loss: \n",
      "Training: 1.752011930878384 Test: 0.3833711932911093\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8306611332924159 Test: 0.33530930626212085\n",
      "Epoch: 204\n",
      "Loss: \n",
      "Training: 1.8219161329409137 Test: 0.3261867663831747\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8435361389099665 Test: 0.3428960698052809\n",
      "Epoch: 205\n",
      "Loss: \n",
      "Training: 1.5694880391232113 Test: 0.30986058234556496\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8382531538371594 Test: 0.33679690189667433\n",
      "Epoch: 206\n",
      "Loss: \n",
      "Training: 2.147387362311768 Test: 0.31334602304493875\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8074413713278168 Test: 0.33426975399322467\n",
      "Epoch: 207\n",
      "Loss: \n",
      "Training: 1.5670988595470048 Test: 0.3701431467754545\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8174978121293257 Test: 0.32948835028931506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 208\n",
      "Loss: \n",
      "Training: 1.7322354282275392 Test: 0.3020865638889905\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.806274672206433 Test: 0.3281859724864873\n",
      "Epoch: 209\n",
      "Loss: \n",
      "Training: 1.7637998157479629 Test: 0.28344226231727815\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7958719322504941 Test: 0.32486470526717237\n",
      "Epoch: 210\n",
      "Loss: \n",
      "Training: 1.7021691597941107 Test: 0.3366033390735329\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7849667725604696 Test: 0.3239280110705509\n",
      "Epoch: 211\n",
      "Loss: \n",
      "Training: 1.8253611406943295 Test: 0.29925145072154047\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.789565698158837 Test: 0.32850418114227314\n",
      "Epoch: 212\n",
      "Loss: \n",
      "Training: 1.6445046053206016 Test: 0.32404920125648257\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7728993182209933 Test: 0.3281227165111003\n",
      "Epoch: 213\n",
      "Loss: \n",
      "Training: 1.7567471698669033 Test: 0.32535303129009774\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7525972474585825 Test: 0.3248340529098067\n",
      "Epoch: 214\n",
      "Loss: \n",
      "Training: 1.977702448282502 Test: 0.2946939659034074\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7530707713574345 Test: 0.31903223670970554\n",
      "Epoch: 215\n",
      "Loss: \n",
      "Training: 1.732895087323233 Test: 0.3588239394125393\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7686494028915933 Test: 0.31588295666172883\n",
      "Epoch: 216\n",
      "Loss: \n",
      "Training: 1.6239787549725841 Test: 0.3417484140474371\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7849901077115955 Test: 0.3207792923684263\n",
      "Epoch: 217\n",
      "Loss: \n",
      "Training: 1.515091208058158 Test: 0.3149528692094843\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.732649246977677 Test: 0.3236195314686761\n",
      "Epoch: 218\n",
      "Loss: \n",
      "Training: 1.8770786039085117 Test: 0.3686304470315846\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7274484818287923 Test: 0.3181005037120791\n",
      "Epoch: 219\n",
      "Loss: \n",
      "Training: 2.0033428887893727 Test: 0.30205488392646496\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7419327993968892 Test: 0.32475489202633845\n",
      "Epoch: 220\n",
      "Loss: \n",
      "Training: 1.8667846129871957 Test: 0.3269322989698932\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7658871067010302 Test: 0.3266161541872571\n",
      "Epoch: 221\n",
      "Loss: \n",
      "Training: 1.9500199341182218 Test: 0.3199418257037267\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7823486520203389 Test: 0.32564905017689316\n",
      "Epoch: 222\n",
      "Loss: \n",
      "Training: 1.8802745012829807 Test: 0.34845810945843325\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7948145313627282 Test: 0.3277180876751118\n",
      "Epoch: 223\n",
      "Loss: \n",
      "Training: 2.0718183263921994 Test: 0.33937818595066344\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.818391520958966 Test: 0.33015897849530684\n",
      "Epoch: 224\n",
      "Loss: \n",
      "Training: 1.833987737678518 Test: 0.3194291041327951\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8498986366114962 Test: 0.3315614939613635\n",
      "Epoch: 225\n",
      "Loss: \n",
      "Training: 1.502720760574398 Test: 0.30625935885321753\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8355271655510979 Test: 0.33403500778430223\n",
      "Epoch: 226\n",
      "Loss: \n",
      "Training: 1.628685249009765 Test: 0.33566565294648854\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.812509732876214 Test: 0.32877854972837006\n",
      "Epoch: 227\n",
      "Loss: \n",
      "Training: 1.540809719974035 Test: 0.32924819427902735\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8129803822799322 Test: 0.32817027361827517\n",
      "Epoch: 228\n",
      "Loss: \n",
      "Training: 2.0274524493896338 Test: 0.3351503052513423\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8155522334715202 Test: 0.3295998061252295\n",
      "Epoch: 229\n",
      "Loss: \n",
      "Training: 2.128981758160553 Test: 0.3128275534502236\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8305896180196322 Test: 0.32625179194720527\n",
      "Epoch: 230\n",
      "Loss: \n",
      "Training: 1.8624763859482456 Test: 0.3079142052021152\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.84315350495675 Test: 0.32732905889958114\n",
      "Epoch: 231\n",
      "Loss: \n",
      "Training: 2.026865933910955 Test: 0.37202725902429973\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8427226822528548 Test: 0.32542724952280333\n",
      "Epoch: 232\n",
      "Loss: \n",
      "Training: 1.9776301389008388 Test: 0.35652301292479094\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8504072822321285 Test: 0.3306357928548606\n",
      "Epoch: 233\n",
      "Loss: \n",
      "Training: 1.836934808178111 Test: 0.3308249083693628\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.860142845993914 Test: 0.3314422832014964\n",
      "Epoch: 234\n",
      "Loss: \n",
      "Training: 2.1096712958305814 Test: 0.304657572226678\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.836654494172505 Test: 0.3305869554433663\n",
      "Epoch: 235\n",
      "Loss: \n",
      "Training: 1.9616183518625803 Test: 0.3082958445247322\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8642228499877114 Test: 0.3291098022527546\n",
      "Epoch: 236\n",
      "Loss: \n",
      "Training: 1.549977554838939 Test: 0.3055973177201502\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9101126091165295 Test: 0.329313450819906\n",
      "Epoch: 237\n",
      "Loss: \n",
      "Training: 1.9003412605741044 Test: 0.3133662085046898\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9022418396994474 Test: 0.3263066172972723\n",
      "Epoch: 238\n",
      "Loss: \n",
      "Training: 1.7575563334708673 Test: 0.3012729762180945\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.938194993759454 Test: 0.32471841871983853\n",
      "Epoch: 239\n",
      "Loss: \n",
      "Training: 1.8634686743843323 Test: 0.3782267923659301\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9112053821675776 Test: 0.3213306858165137\n",
      "Epoch: 240\n",
      "Loss: \n",
      "Training: 1.872120547310463 Test: 0.31614854034293827\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8846540737899555 Test: 0.3278706097080843\n",
      "Epoch: 241\n",
      "Loss: \n",
      "Training: 1.9975589506436626 Test: 0.3212950610698647\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.885618489926177 Test: 0.3286940432221666\n",
      "Epoch: 242\n",
      "Loss: \n",
      "Training: 1.874215797559142 Test: 0.32265008355877645\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.882687791599448 Test: 0.3236208234267231\n",
      "Epoch: 243\n",
      "Loss: \n",
      "Training: 1.9350600193605985 Test: 0.31261770289385576\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8723463574652783 Test: 0.3202335304901217\n",
      "Epoch: 244\n",
      "Loss: \n",
      "Training: 2.1110437858710065 Test: 0.33132631685665853\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.882158878583527 Test: 0.31841280994257104\n",
      "Epoch: 245\n",
      "Loss: \n",
      "Training: 1.5874926672117051 Test: 0.3560271993788987\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8822961275875696 Test: 0.32107968440556905\n",
      "Epoch: 246\n",
      "Loss: \n",
      "Training: 2.001245509836927 Test: 0.35749334680168565\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8448835591224821 Test: 0.3258528198909857\n",
      "Epoch: 247\n",
      "Loss: \n",
      "Training: 1.9728180242977844 Test: 0.30498184668679523\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8900103546222808 Test: 0.3310424227991392\n",
      "Epoch: 248\n",
      "Loss: \n",
      "Training: 1.8888319074670332 Test: 0.31258771940575614\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8972580309946487 Test: 0.3302039866173498\n",
      "Epoch: 249\n",
      "Loss: \n",
      "Training: 1.8398338084456416 Test: 0.35981844188645806\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9103855883942653 Test: 0.33133546093611593\n",
      "Epoch: 250\n",
      "Loss: \n",
      "Training: 1.9881390769346616 Test: 0.32641944663356326\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.908022101800396 Test: 0.3294946258881687\n",
      "Epoch: 251\n",
      "Loss: \n",
      "Training: 1.943535797527591 Test: 0.31651345446021684\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9196239547628164 Test: 0.3305217165172312\n",
      "Epoch: 252\n",
      "Loss: \n",
      "Training: 1.5658906181784613 Test: 0.3748186824602759\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9142216394512093 Test: 0.3300435558562665\n",
      "Epoch: 253\n",
      "Loss: \n",
      "Training: 1.8290968189166423 Test: 0.37500582616705086\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8833891215131409 Test: 0.3352604157464164\n",
      "Epoch: 254\n",
      "Loss: \n",
      "Training: 2.1081482437838646 Test: 0.31709467347491277\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8727928014687456 Test: 0.3414992280737359\n",
      "Epoch: 255\n",
      "Loss: \n",
      "Training: 1.6064066399835535 Test: 0.315254574220194\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8725032472600311 Test: 0.3400760637355614\n",
      "Epoch: 256\n",
      "Loss: \n",
      "Training: 1.8271737396924785 Test: 0.3781512260577144\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8743946445372164 Test: 0.3359988012196909\n",
      "Epoch: 257\n",
      "Loss: \n",
      "Training: 1.8106219202601657 Test: 0.2967663273601703\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8569874675227713 Test: 0.3380645891452937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 258\n",
      "Loss: \n",
      "Training: 1.8595304353035131 Test: 0.3359681144923556\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8407678571190094 Test: 0.33724303721263127\n",
      "Epoch: 259\n",
      "Loss: \n",
      "Training: 2.014794100529169 Test: 0.3086030336556192\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8378377099026573 Test: 0.3395810767212912\n",
      "Epoch: 260\n",
      "Loss: \n",
      "Training: 2.0318840890444676 Test: 0.31268385443751656\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.85533373911101 Test: 0.3344595358982073\n",
      "Epoch: 261\n",
      "Loss: \n",
      "Training: 1.817160282234068 Test: 0.3791112827561351\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8597082403219907 Test: 0.3330859766786026\n",
      "Epoch: 262\n",
      "Loss: \n",
      "Training: 1.9848016427503754 Test: 0.3322354138073845\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8470706887926382 Test: 0.33934575950819446\n",
      "Epoch: 263\n",
      "Loss: \n",
      "Training: 1.706139730673064 Test: 0.36058092472580044\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8889617912498298 Test: 0.3350874326429053\n",
      "Epoch: 264\n",
      "Loss: \n",
      "Training: 1.7754165322281361 Test: 0.3214763282835985\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8766660824254722 Test: 0.3336449424987803\n",
      "Epoch: 265\n",
      "Loss: \n",
      "Training: 1.702916359193985 Test: 0.33589653945285597\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.843392911269899 Test: 0.33408310797964885\n",
      "Epoch: 266\n",
      "Loss: \n",
      "Training: 2.114096966107801 Test: 0.31846575805552657\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8530438831909422 Test: 0.3361473045029151\n",
      "Epoch: 267\n",
      "Loss: \n",
      "Training: 2.079070712075985 Test: 0.3016202551927544\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8817362058324743 Test: 0.33017875770269633\n",
      "Epoch: 268\n",
      "Loss: \n",
      "Training: 1.838539728261099 Test: 0.3062374734522251\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9085810850140565 Test: 0.3306641504859547\n",
      "Epoch: 269\n",
      "Loss: \n",
      "Training: 1.729736718436175 Test: 0.3102324441299662\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9064820143098153 Test: 0.32769108638194167\n",
      "Epoch: 270\n",
      "Loss: \n",
      "Training: 1.6414286454278966 Test: 0.34641993092242157\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8779762761005159 Test: 0.32785402742937636\n",
      "Epoch: 271\n",
      "Loss: \n",
      "Training: 1.6616353957150416 Test: 0.32553938997311527\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8389307317388586 Test: 0.3312276350778668\n",
      "Epoch: 272\n",
      "Loss: \n",
      "Training: 1.9096929762409345 Test: 0.31003378458035474\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8233782430869556 Test: 0.3258704457995648\n",
      "Epoch: 273\n",
      "Loss: \n",
      "Training: 1.8539191883064985 Test: 0.3033262298465082\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8158673764360116 Test: 0.3236502828768618\n",
      "Epoch: 274\n",
      "Loss: \n",
      "Training: 1.9015133410194591 Test: 0.3105903879623605\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8306453221993553 Test: 0.31792481338893264\n",
      "Epoch: 275\n",
      "Loss: \n",
      "Training: 1.980262150135898 Test: 0.33346218734269784\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8432550030784873 Test: 0.31683621935680883\n",
      "Epoch: 276\n",
      "Loss: \n",
      "Training: 1.8351491356673264 Test: 0.3244619889313008\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8709895821726785 Test: 0.31659278414579306\n",
      "Epoch: 277\n",
      "Loss: \n",
      "Training: 1.8931531241095751 Test: 0.2944126548203756\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8430947991286313 Test: 0.31719240723337044\n",
      "Epoch: 278\n",
      "Loss: \n",
      "Training: 1.770785640672208 Test: 0.3697590544298498\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8245030403319902 Test: 0.3164716471961325\n",
      "Epoch: 279\n",
      "Loss: \n",
      "Training: 1.8702824557389657 Test: 0.3090153444139447\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8177276315731015 Test: 0.32282380529389504\n",
      "Epoch: 280\n",
      "Loss: \n",
      "Training: 1.9760468491038483 Test: 0.3624684269453343\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8317822053033805 Test: 0.3227020953222929\n",
      "Epoch: 281\n",
      "Loss: \n",
      "Training: 1.9822095608270036 Test: 0.36358151827348495\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8652440256709757 Test: 0.3243069449245842\n",
      "Epoch: 282\n",
      "Loss: \n",
      "Training: 1.8879526796528852 Test: 0.27820189220210917\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8973014421821717 Test: 0.32811115775462113\n",
      "Epoch: 283\n",
      "Loss: \n",
      "Training: 2.0557755635227504 Test: 0.3521660719997861\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8951274125233666 Test: 0.3249279685167966\n",
      "Epoch: 284\n",
      "Loss: \n",
      "Training: 1.837734524531536 Test: 0.32413095178812174\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.915313050044992 Test: 0.3298119527321244\n",
      "Epoch: 285\n",
      "Loss: \n",
      "Training: 1.6038025357582435 Test: 0.31816467702833107\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9089351683961997 Test: 0.3311660091147005\n",
      "Epoch: 286\n",
      "Loss: \n",
      "Training: 1.9859974962044602 Test: 0.36724648570598445\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8712892069584346 Test: 0.3296362580832638\n",
      "Epoch: 287\n",
      "Loss: \n",
      "Training: 1.676950269704686 Test: 0.31911520920830355\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8863740430121478 Test: 0.3339147077607322\n",
      "Epoch: 288\n",
      "Loss: \n",
      "Training: 1.941177619477536 Test: 0.3269428292559078\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8647537575716586 Test: 0.336384963199525\n",
      "Epoch: 289\n",
      "Loss: \n",
      "Training: 1.98495804541542 Test: 0.31081138124674335\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8817929554521915 Test: 0.3321033406821308\n",
      "Epoch: 290\n",
      "Loss: \n",
      "Training: 1.5636407964908958 Test: 0.3278594282429908\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.893260514419837 Test: 0.3322829443654106\n",
      "Epoch: 291\n",
      "Loss: \n",
      "Training: 1.7770146775483704 Test: 0.3217347499499965\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8520199091585419 Test: 0.32882204449517627\n",
      "Epoch: 292\n",
      "Loss: \n",
      "Training: 1.8439092193265234 Test: 0.31518879961034973\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8315004208306782 Test: 0.32463736766282747\n",
      "Epoch: 293\n",
      "Loss: \n",
      "Training: 1.8756053971868318 Test: 0.3082811275958649\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.827096074798042 Test: 0.32833605840365154\n",
      "Epoch: 294\n",
      "Loss: \n",
      "Training: 1.7736524873347044 Test: 0.29337547209499565\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8090790581644502 Test: 0.3239475639632594\n",
      "Epoch: 295\n",
      "Loss: \n",
      "Training: 1.9400995290063001 Test: 0.36796210030883697\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8026708544447672 Test: 0.32087201599394677\n",
      "Epoch: 296\n",
      "Loss: \n",
      "Training: 1.864557594851736 Test: 0.29604414407851726\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8363005537695727 Test: 0.3258517583219974\n",
      "Epoch: 297\n",
      "Loss: \n",
      "Training: 1.85419640063516 Test: 0.31373959325193057\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8241565636343 Test: 0.31873152415925066\n",
      "Epoch: 298\n",
      "Loss: \n",
      "Training: 1.8242132360109318 Test: 0.32964540320561475\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8418811767273477 Test: 0.3181939625636134\n",
      "Epoch: 299\n",
      "Loss: \n",
      "Training: 1.5456716413501912 Test: 0.35320257680444567\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8301847383806868 Test: 0.3184642199585841\n",
      "Epoch: 300\n",
      "Loss: \n",
      "Training: 1.857853467282692 Test: 0.3289513080184432\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7862560979741644 Test: 0.32270333951435426\n",
      "Epoch: 301\n",
      "Loss: \n",
      "Training: 1.6508986138780897 Test: 0.3178673233246429\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8156773650533442 Test: 0.3228125274918995\n",
      "Epoch: 302\n",
      "Loss: \n",
      "Training: 1.94479297365143 Test: 0.32245406532979765\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8030657586863164 Test: 0.3224257848293641\n",
      "Epoch: 303\n",
      "Loss: \n",
      "Training: 1.7631017349737759 Test: 0.3235615176088302\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.813154134118807 Test: 0.32315231140130896\n",
      "Epoch: 304\n",
      "Loss: \n",
      "Training: 1.8250184728256165 Test: 0.3036324187469802\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.801903767897501 Test: 0.32468035040260546\n",
      "Epoch: 305\n",
      "Loss: \n",
      "Training: 1.7145473807741634 Test: 0.3250222025675276\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8070403664465922 Test: 0.3257060450678039\n",
      "Epoch: 306\n",
      "Loss: \n",
      "Training: 1.7959257869423928 Test: 0.3152266577834068\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7844851516233784 Test: 0.321412055293673\n",
      "Epoch: 307\n",
      "Loss: \n",
      "Training: 1.8597577936967828 Test: 0.3136905431440478\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7776219708324443 Test: 0.32333030666416196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 308\n",
      "Loss: \n",
      "Training: 1.9096160170172993 Test: 0.29227351392751444\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7781781101386067 Test: 0.3233254016533737\n",
      "Epoch: 309\n",
      "Loss: \n",
      "Training: 2.101639495770292 Test: 0.312337313895706\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7867183882392434 Test: 0.3195882127255637\n",
      "Epoch: 310\n",
      "Loss: \n",
      "Training: 1.7451818256259801 Test: 0.3807559660756661\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8423151736812535 Test: 0.3155016864346897\n",
      "Epoch: 311\n",
      "Loss: \n",
      "Training: 1.9204879899337666 Test: 0.36571166867376015\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8310480095155826 Test: 0.3206821522404121\n",
      "Epoch: 312\n",
      "Loss: \n",
      "Training: 1.9297920611984933 Test: 0.31021398814188533\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8580069471211502 Test: 0.32546658677532375\n",
      "Epoch: 313\n",
      "Loss: \n",
      "Training: 1.9465545145645462 Test: 0.3007815995286967\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8565068558758564 Test: 0.3242425790565325\n",
      "Epoch: 314\n",
      "Loss: \n",
      "Training: 1.9644779314441767 Test: 0.34475633112941967\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8748521338349335 Test: 0.3219645872485191\n",
      "Epoch: 315\n",
      "Loss: \n",
      "Training: 2.0033970644956485 Test: 0.37742580110014834\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8887980796967891 Test: 0.32607697848676304\n",
      "Epoch: 316\n",
      "Loss: \n",
      "Training: 1.755132119326589 Test: 0.30682703334475225\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9176830480689375 Test: 0.3313173383400251\n",
      "Epoch: 317\n",
      "Loss: \n",
      "Training: 1.8701833195219089 Test: 0.32813978815616063\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9136036813073578 Test: 0.33047737589615966\n",
      "Epoch: 318\n",
      "Loss: \n",
      "Training: 2.0076177539187077 Test: 0.3667484454656614\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9146462338898704 Test: 0.3319223003973709\n",
      "Epoch: 319\n",
      "Loss: \n",
      "Training: 1.806370065063783 Test: 0.30296352635275114\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.924446407580011 Test: 0.3393697935511856\n",
      "Epoch: 320\n",
      "Loss: \n",
      "Training: 2.040125950730387 Test: 0.3706154331827187\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8949194645093599 Test: 0.3384324147968902\n",
      "Epoch: 321\n",
      "Loss: \n",
      "Training: 1.6702976503583158 Test: 0.30119850651507624\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.924413877019801 Test: 0.3374183615075954\n",
      "Epoch: 322\n",
      "Loss: \n",
      "Training: 1.8078142206056667 Test: 0.3189575750752259\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8993948430622551 Test: 0.33096704529172705\n",
      "Epoch: 323\n",
      "Loss: \n",
      "Training: 1.7922789182623142 Test: 0.3521889890139116\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.887197059002973 Test: 0.3318414039850611\n",
      "Epoch: 324\n",
      "Loss: \n",
      "Training: 2.071473655434792 Test: 0.3157766279730922\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8717694993727498 Test: 0.33698214293358253\n",
      "Epoch: 325\n",
      "Loss: \n",
      "Training: 1.8068066961647529 Test: 0.31147144182560704\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8824690717718113 Test: 0.33408417261794987\n",
      "Epoch: 326\n",
      "Loss: \n",
      "Training: 2.0845026460560105 Test: 0.30112852773834436\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8628100349387218 Test: 0.32748873669049566\n",
      "Epoch: 327\n",
      "Loss: \n",
      "Training: 2.0767731258673106 Test: 0.3436067238566299\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8957470876116638 Test: 0.3269188861298549\n",
      "Epoch: 328\n",
      "Loss: \n",
      "Training: 1.8672355288580602 Test: 0.3007267605360045\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9164060682462036 Test: 0.3284655796999018\n",
      "Epoch: 329\n",
      "Loss: \n",
      "Training: 1.959506840399883 Test: 0.3277099516186424\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9023678457401396 Test: 0.3218634112069362\n",
      "Epoch: 330\n",
      "Loss: \n",
      "Training: 1.7300452506496218 Test: 0.31588218108685223\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9176815232737492 Test: 0.3243380537335253\n",
      "Epoch: 331\n",
      "Loss: \n",
      "Training: 1.8513876931010163 Test: 0.29645759638555663\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8866734532656728 Test: 0.31886472852393866\n",
      "Epoch: 332\n",
      "Loss: \n",
      "Training: 1.9821328786107977 Test: 0.3118542347317079\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9047824575399428 Test: 0.31839063751098673\n",
      "Epoch: 333\n",
      "Loss: \n",
      "Training: 2.0499913116431845 Test: 0.3327598686032674\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9222143233404558 Test: 0.31768030347663495\n",
      "Epoch: 334\n",
      "Loss: \n",
      "Training: 1.7761076436802257 Test: 0.3172719631013953\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.947985562678543 Test: 0.31573739143557045\n",
      "Epoch: 335\n",
      "Loss: \n",
      "Training: 1.8176241741316552 Test: 0.3886276001877593\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9184489615030862 Test: 0.31588692494840076\n",
      "Epoch: 336\n",
      "Loss: \n",
      "Training: 1.8926562725154628 Test: 0.3263247917363475\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9195307092997766 Test: 0.32360254078461603\n",
      "Epoch: 337\n",
      "Loss: \n",
      "Training: 2.0105092307803374 Test: 0.31018835793513755\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.900346071945722 Test: 0.3261221671844164\n",
      "Epoch: 338\n",
      "Loss: \n",
      "Training: 1.7117650934833568 Test: 0.31315111827151315\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8937196824370244 Test: 0.32278033059226713\n",
      "Epoch: 339\n",
      "Loss: \n",
      "Training: 1.6550055104004306 Test: 0.32726015763290667\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.878172638899554 Test: 0.32402276636581795\n",
      "Epoch: 340\n",
      "Loss: \n",
      "Training: 1.9574510839232826 Test: 0.3039553031710506\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.847722505899609 Test: 0.32397778696724433\n",
      "Epoch: 341\n",
      "Loss: \n",
      "Training: 2.043300715511418 Test: 0.36635841799457286\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8704630892269751 Test: 0.32278509917566417\n",
      "Epoch: 342\n",
      "Loss: \n",
      "Training: 2.0243796353473704 Test: 0.35985543660798486\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.889654391468015 Test: 0.3297751813365658\n",
      "Epoch: 343\n",
      "Loss: \n",
      "Training: 1.7906956760654171 Test: 0.37336390510567147\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8938790671416723 Test: 0.33457530152419357\n",
      "Epoch: 344\n",
      "Loss: \n",
      "Training: 1.511609534009355 Test: 0.3524246506042669\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8679495035838958 Test: 0.3386357051744339\n",
      "Epoch: 345\n",
      "Loss: \n",
      "Training: 2.101591393465433 Test: 0.33322002062169065\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8414996926168086 Test: 0.3421509739247211\n",
      "Epoch: 346\n",
      "Loss: \n",
      "Training: 1.823764546813448 Test: 0.330526895152003\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8698964145501864 Test: 0.33661021596811425\n",
      "Epoch: 347\n",
      "Loss: \n",
      "Training: 2.0269767926997284 Test: 0.2992812534700998\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8630072419799848 Test: 0.3370304263096798\n",
      "Epoch: 348\n",
      "Loss: \n",
      "Training: 1.6361762588120503 Test: 0.30878508049131403\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8646539981719237 Test: 0.33593971586317595\n",
      "Epoch: 349\n",
      "Loss: \n",
      "Training: 2.096798824817119 Test: 0.3172025069665735\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8570951147047936 Test: 0.33550311208515604\n",
      "Epoch: 350\n",
      "Loss: \n",
      "Training: 1.8448301968358467 Test: 0.3161091959857069\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9012744461464621 Test: 0.33449734701852274\n",
      "Epoch: 351\n",
      "Loss: \n",
      "Training: 1.740656972554393 Test: 0.3282801798748327\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8900123574377186 Test: 0.3357127362999884\n",
      "Epoch: 352\n",
      "Loss: \n",
      "Training: 1.4691301024774783 Test: 0.30426175974472947\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8597479831420158 Test: 0.33190491248801435\n",
      "Epoch: 353\n",
      "Loss: \n",
      "Training: 1.9122619461329269 Test: 0.3301254446637283\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8042230298550268 Test: 0.3263455448016888\n",
      "Epoch: 354\n",
      "Loss: \n",
      "Training: 1.790026965230334 Test: 0.3366550951568426\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.816379656861778 Test: 0.32202169875749453\n",
      "Epoch: 355\n",
      "Loss: \n",
      "Training: 1.8077966393050047 Test: 0.32248901405185615\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8442213999838757 Test: 0.3204447432127522\n",
      "Epoch: 356\n",
      "Loss: \n",
      "Training: 1.7816235225143056 Test: 0.31218818006707894\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8148419245678329 Test: 0.31937164255576866\n",
      "Epoch: 357\n",
      "Loss: \n",
      "Training: 1.856631082060655 Test: 0.3240597423555307\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8106278221379188 Test: 0.31753777104727626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 358\n",
      "Loss: \n",
      "Training: 1.7579542426211359 Test: 0.3059223133969754\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7935932510740116 Test: 0.3200156199358194\n",
      "Epoch: 359\n",
      "Loss: \n",
      "Training: 1.8731243642396556 Test: 0.37446293997695873\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8057710494549197 Test: 0.31972934322638547\n",
      "Epoch: 360\n",
      "Loss: \n",
      "Training: 1.8218587573973803 Test: 0.34055672557767386\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7834036033971734 Test: 0.325455386527424\n",
      "Epoch: 361\n",
      "Loss: \n",
      "Training: 2.058992496298224 Test: 0.3016647705684341\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.781106459453327 Test: 0.32790013948662067\n",
      "Epoch: 362\n",
      "Loss: \n",
      "Training: 2.0106171672462416 Test: 0.3086068452005767\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8129400118277101 Test: 0.3252385985559808\n",
      "Epoch: 363\n",
      "Loss: \n",
      "Training: 1.812193921264492 Test: 0.3002946066572846\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8670887183045863 Test: 0.32567310710156555\n",
      "Epoch: 364\n",
      "Loss: \n",
      "Training: 2.1268024661402056 Test: 0.3172794088718415\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.857081915817743 Test: 0.32269002330092117\n",
      "Epoch: 365\n",
      "Loss: \n",
      "Training: 2.012760029605998 Test: 0.3370807029301738\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8907594659087301 Test: 0.3207524546724211\n",
      "Epoch: 366\n",
      "Loss: \n",
      "Training: 1.9076991541191453 Test: 0.36977820267589206\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9112558049388295 Test: 0.3222116235602528\n",
      "Epoch: 367\n",
      "Loss: \n",
      "Training: 1.9312532981367054 Test: 0.3092740652734352\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9238633680993136 Test: 0.3279706258211341\n",
      "Epoch: 368\n",
      "Loss: \n",
      "Training: 1.76995272692972 Test: 0.36987434452824663\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9313255897069184 Test: 0.3264920581129246\n",
      "Epoch: 369\n",
      "Loss: \n",
      "Training: 1.5707895573095578 Test: 0.31298084149166555\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9325254381377768 Test: 0.3328872612260517\n",
      "Epoch: 370\n",
      "Loss: \n",
      "Training: 1.7408003527547324 Test: 0.31698476791737573\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9022919574447665 Test: 0.32673905137752246\n",
      "Epoch: 371\n",
      "Loss: \n",
      "Training: 1.6050104448975269 Test: 0.32014264386697633\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8941861169805019 Test: 0.3243818556114926\n",
      "Epoch: 372\n",
      "Loss: \n",
      "Training: 1.6502286707763003 Test: 0.30947463173129774\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8487879118404327 Test: 0.3262296429413468\n",
      "Epoch: 373\n",
      "Loss: \n",
      "Training: 1.9902298117919963 Test: 0.36136146216114473\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8127490621934386 Test: 0.3263164215944189\n",
      "Epoch: 374\n",
      "Loss: \n",
      "Training: 1.880252128500415 Test: 0.34471478851295023\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8305526512461885 Test: 0.33242310714480494\n",
      "Epoch: 375\n",
      "Loss: \n",
      "Training: 1.854617246250592 Test: 0.32074929966070415\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.80589761748221 Test: 0.3351666451089158\n",
      "Epoch: 376\n",
      "Loss: \n",
      "Training: 1.836824256123303 Test: 0.32778105854150225\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.790083339146669 Test: 0.33353350478196886\n",
      "Epoch: 377\n",
      "Loss: \n",
      "Training: 1.7066265237351355 Test: 0.3225587625789409\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7829958493470848 Test: 0.32933379036852983\n",
      "Epoch: 378\n",
      "Loss: \n",
      "Training: 1.5118688584051982 Test: 0.3155271765500416\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.760533171906928 Test: 0.3306622600990804\n",
      "Epoch: 379\n",
      "Loss: \n",
      "Training: 1.7976337232834594 Test: 0.38513354821396845\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7347247850544758 Test: 0.3252275433012599\n",
      "Epoch: 380\n",
      "Loss: \n",
      "Training: 2.016794184160451 Test: 0.3287401293564805\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7574092016518659 Test: 0.33244281397349024\n",
      "Epoch: 381\n",
      "Loss: \n",
      "Training: 1.6311424902995282 Test: 0.37383188462544203\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7850085847924375 Test: 0.3336183501174007\n",
      "Epoch: 382\n",
      "Loss: \n",
      "Training: 1.847299489963997 Test: 0.37666225164320466\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7876217893326376 Test: 0.33898727419324726\n",
      "Epoch: 383\n",
      "Loss: \n",
      "Training: 1.9029236015744864 Test: 0.31187311917991656\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8073288712514077 Test: 0.34570603618443796\n",
      "Epoch: 384\n",
      "Loss: \n",
      "Training: 1.7845600812797395 Test: 0.3573556380440951\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7985982502296565 Test: 0.34075720188631514\n",
      "Epoch: 385\n",
      "Loss: \n",
      "Training: 1.9863959771645407 Test: 0.3675721450625075\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7890290455075888 Test: 0.34202128683942956\n",
      "Epoch: 386\n",
      "Loss: \n",
      "Training: 2.0125756290631522 Test: 0.3145105005064509\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.802206918598984 Test: 0.3467035713796099\n",
      "Epoch: 387\n",
      "Loss: \n",
      "Training: 1.7307193118267117 Test: 0.359148787554015\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.819782055892969 Test: 0.34537651557610477\n",
      "Epoch: 388\n",
      "Loss: \n",
      "Training: 1.7892699921530126 Test: 0.3669989740438698\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8221913347021264 Test: 0.34903551807361216\n",
      "Epoch: 389\n",
      "Loss: \n",
      "Training: 1.855196605799823 Test: 0.3739882466705879\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.849931448076908 Test: 0.354182697822995\n",
      "Epoch: 390\n",
      "Loss: \n",
      "Training: 2.006340459576648 Test: 0.34081936580847444\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.855687736328544 Test: 0.35306816766865695\n",
      "Epoch: 391\n",
      "Loss: \n",
      "Training: 1.7287523050064413 Test: 0.31377156105428167\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.854642363870164 Test: 0.3542760913138564\n",
      "Epoch: 392\n",
      "Loss: \n",
      "Training: 2.124460584168624 Test: 0.32897949818865074\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8644033453408553 Test: 0.34827005895674035\n",
      "Epoch: 393\n",
      "Loss: \n",
      "Training: 1.79602624511624 Test: 0.3913708993734778\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.892119454761318 Test: 0.3435017836112849\n",
      "Epoch: 394\n",
      "Loss: \n",
      "Training: 1.9736729478096278 Test: 0.30713712615672356\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8814297191154936 Test: 0.35145156163064106\n",
      "Epoch: 395\n",
      "Loss: \n",
      "Training: 1.9258034645718245 Test: 0.3405235428087861\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9003410057684824 Test: 0.34642971044190396\n",
      "Epoch: 396\n",
      "Loss: \n",
      "Training: 1.7170984152467799 Test: 0.29089827801608836\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8942817545092105 Test: 0.34372485021653176\n",
      "Epoch: 397\n",
      "Loss: \n",
      "Training: 2.113589073796932 Test: 0.35522124695689644\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8647340331275735 Test: 0.34136362796749553\n",
      "Epoch: 398\n",
      "Loss: \n",
      "Training: 2.0527792318544322 Test: 0.3102038710637964\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9030210093245952 Test: 0.3409708739077837\n",
      "Epoch: 399\n",
      "Loss: \n",
      "Training: 1.9062818604568297 Test: 0.34562722880754915\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9293719332947372 Test: 0.33529136360977635\n",
      "Epoch: 400\n",
      "Loss: \n",
      "Training: 1.8427455348001367 Test: 0.3140281542289398\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9344804587604378 Test: 0.33245526182347246\n",
      "Epoch: 401\n",
      "Loss: \n",
      "Training: 1.8234146677137302 Test: 0.36656877784469527\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9181209662827867 Test: 0.32977614066551897\n",
      "Epoch: 402\n",
      "Loss: \n",
      "Training: 2.0491354167010964 Test: 0.3676022979312338\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9275872025535157 Test: 0.33505586234456036\n",
      "Epoch: 403\n",
      "Loss: \n",
      "Training: 1.9801338867975238 Test: 0.2872601895057315\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9200546858067633 Test: 0.33891814231881867\n",
      "Epoch: 404\n",
      "Loss: \n",
      "Training: 1.705690170276819 Test: 0.3234010827724372\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9384654499748912 Test: 0.32850707133204404\n",
      "Epoch: 405\n",
      "Loss: \n",
      "Training: 1.7840255159398366 Test: 0.31947617839442094\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9116671722216103 Test: 0.33013346699361545\n",
      "Epoch: 406\n",
      "Loss: \n",
      "Training: 1.6999045948496128 Test: 0.303320383764883\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8974893773584114 Test: 0.32802873055217885\n",
      "Epoch: 407\n",
      "Loss: \n",
      "Training: 2.008000739050013 Test: 0.36566470919767924\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.895769995318695 Test: 0.32927094112705835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 408\n",
      "Loss: \n",
      "Training: 1.9339427915914214 Test: 0.3550358089564872\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8852111618440035 Test: 0.33031528735113663\n",
      "Epoch: 409\n",
      "Loss: \n",
      "Training: 2.002435036168609 Test: 0.3889409929279025\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8733275178177018 Test: 0.33479848114040567\n",
      "Epoch: 410\n",
      "Loss: \n",
      "Training: 1.8060789979401675 Test: 0.33537150697939033\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8829428353888797 Test: 0.33912985755244096\n",
      "Epoch: 411\n",
      "Loss: \n",
      "Training: 1.726346792673428 Test: 0.33429465276168746\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8792761817028834 Test: 0.3412641928274861\n",
      "Epoch: 412\n",
      "Loss: \n",
      "Training: 1.8264012466856112 Test: 0.31539523578959816\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.869569394198853 Test: 0.3380367803191853\n",
      "Epoch: 413\n",
      "Loss: \n",
      "Training: 1.7127318837989358 Test: 0.3040417467223189\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8472959771973045 Test: 0.33281607410502173\n",
      "Epoch: 414\n",
      "Loss: \n",
      "Training: 1.931042797127917 Test: 0.3149567432220123\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8205557768974454 Test: 0.33449422982668053\n",
      "Epoch: 415\n",
      "Loss: \n",
      "Training: 1.7718099460042096 Test: 0.34848849306733043\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8430910395825553 Test: 0.333649795871638\n",
      "Epoch: 416\n",
      "Loss: \n",
      "Training: 1.8065728051938756 Test: 0.3296273943202894\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8418694825889925 Test: 0.33655102733892894\n",
      "Epoch: 417\n",
      "Loss: \n",
      "Training: 1.8451454087142527 Test: 0.31741404586056177\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.852536303623419 Test: 0.3391817283944696\n",
      "Epoch: 418\n",
      "Loss: \n",
      "Training: 2.006991776392358 Test: 0.3732711089678638\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8362507705898428 Test: 0.33435666206075787\n",
      "Epoch: 419\n",
      "Loss: \n",
      "Training: 1.9077919456171828 Test: 0.2990224584094416\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8435556690699364 Test: 0.3361801920618956\n",
      "Epoch: 420\n",
      "Loss: \n",
      "Training: 1.7753728196247172 Test: 0.3094680237143625\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.834091360014794 Test: 0.3271883386100494\n",
      "Epoch: 421\n",
      "Loss: \n",
      "Training: 1.8744763954965034 Test: 0.37352005669909605\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8310207421832487 Test: 0.32459799028354663\n",
      "Epoch: 422\n",
      "Loss: \n",
      "Training: 2.011383451676106 Test: 0.3138242728523476\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8458337024655562 Test: 0.3285205306772875\n",
      "Epoch: 423\n",
      "Loss: \n",
      "Training: 1.7375609954705817 Test: 0.3172749769206679\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8643319229646056 Test: 0.32836343438356247\n",
      "Epoch: 424\n",
      "Loss: \n",
      "Training: 1.8817241421241786 Test: 0.30880681889731204\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.86681483413177 Test: 0.32968675740339737\n",
      "Epoch: 425\n",
      "Loss: \n",
      "Training: 1.6889987469042604 Test: 0.3127934844661472\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8618829686313965 Test: 0.32907176497092727\n",
      "Epoch: 426\n",
      "Loss: \n",
      "Training: 1.8096456057425534 Test: 0.37395479782456503\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8536018487214014 Test: 0.325502264110809\n",
      "Epoch: 427\n",
      "Loss: \n",
      "Training: 1.8634865746703995 Test: 0.36117915494148767\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8539091287762695 Test: 0.3299350044612365\n",
      "Epoch: 428\n",
      "Loss: \n",
      "Training: 2.028940985332822 Test: 0.37989155841688216\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8557432453718845 Test: 0.3343115153693291\n",
      "Epoch: 429\n",
      "Loss: \n",
      "Training: 1.923859704110607 Test: 0.32259736918983406\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8579381662659302 Test: 0.334973560314231\n",
      "Epoch: 430\n",
      "Loss: \n",
      "Training: 1.8846523244547149 Test: 0.34167734961013546\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8595449421152732 Test: 0.33733105139227015\n",
      "Epoch: 431\n",
      "Loss: \n",
      "Training: 1.8856536977509801 Test: 0.3018754284820109\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8704728925982728 Test: 0.3405519839818475\n",
      "Epoch: 432\n",
      "Loss: \n",
      "Training: 1.6169859879990596 Test: 0.30698683913947206\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8715906228237202 Test: 0.333387521160139\n",
      "Epoch: 433\n",
      "Loss: \n",
      "Training: 1.9380621705906524 Test: 0.3211859533059495\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8321508764560157 Test: 0.3327037777888514\n",
      "Epoch: 434\n",
      "Loss: \n",
      "Training: 1.9912406541485383 Test: 0.3070942842230005\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8522009939680228 Test: 0.3330948754273796\n",
      "Epoch: 435\n",
      "Loss: \n",
      "Training: 1.860952564966611 Test: 0.37905568975267345\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8631526451704588 Test: 0.33292362195994846\n",
      "Epoch: 436\n",
      "Loss: \n",
      "Training: 1.8452623101376753 Test: 0.2969292540600517\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8803480269766937 Test: 0.3395498424886011\n",
      "Epoch: 437\n",
      "Loss: \n",
      "Training: 1.9621390209080536 Test: 0.37790265369862414\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8839096974162057 Test: 0.33184728811214975\n",
      "Epoch: 438\n",
      "Loss: \n",
      "Training: 2.1152062916857592 Test: 0.32004046474536774\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8937749420399714 Test: 0.33351963798786344\n",
      "Epoch: 439\n",
      "Loss: \n",
      "Training: 1.879005226037173 Test: 0.33782870391542813\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9024014726752654 Test: 0.327534528620712\n",
      "Epoch: 440\n",
      "Loss: \n",
      "Training: 1.8355842718734092 Test: 0.317742736237554\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8979160248679219 Test: 0.32905766209327136\n",
      "Epoch: 441\n",
      "Loss: \n",
      "Training: 1.7348720318537554 Test: 0.3133278156068958\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.893009219609791 Test: 0.3266642007560132\n",
      "Epoch: 442\n",
      "Loss: \n",
      "Training: 1.9541697005729777 Test: 0.31863729599580837\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8779310530200686 Test: 0.3278094394685017\n",
      "Epoch: 443\n",
      "Loss: \n",
      "Training: 1.844838760982246 Test: 0.3468187553740554\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9116494242774607 Test: 0.32897448515413535\n",
      "Epoch: 444\n",
      "Loss: \n",
      "Training: 1.9632153271708623 Test: 0.30313037179112146\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9023270833166197 Test: 0.331537765360946\n",
      "Epoch: 445\n",
      "Loss: \n",
      "Training: 1.875046616318475 Test: 0.32102789260520237\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.899524550618852 Test: 0.331141374117758\n",
      "Epoch: 446\n",
      "Loss: \n",
      "Training: 1.7272345700966734 Test: 0.2909068190534393\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9009339557540386 Test: 0.325338594403011\n",
      "Epoch: 447\n",
      "Loss: \n",
      "Training: 1.6343554989546327 Test: 0.32883018716897494\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8891311817499385 Test: 0.32473635090234965\n",
      "Epoch: 448\n",
      "Loss: \n",
      "Training: 2.068996243819634 Test: 0.3204582935460347\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8563528295545964 Test: 0.31982910424938477\n",
      "Epoch: 449\n",
      "Loss: \n",
      "Training: 1.9716880245792665 Test: 0.3226126434012264\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.851731824767984 Test: 0.31987088712945144\n",
      "Epoch: 450\n",
      "Loss: \n",
      "Training: 1.644607292951211 Test: 0.3312496346037408\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8610001046221931 Test: 0.3183492810780313\n",
      "Epoch: 451\n",
      "Loss: \n",
      "Training: 2.085623370441191 Test: 0.36825004583135895\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8419024067299734 Test: 0.3196999709146499\n",
      "Epoch: 452\n",
      "Loss: \n",
      "Training: 1.6007221731108965 Test: 0.31414644783786905\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.876977540588717 Test: 0.3251921939370963\n",
      "Epoch: 453\n",
      "Loss: \n",
      "Training: 1.978572115019805 Test: 0.29446209583773514\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.841632787842509 Test: 0.32474310912130233\n",
      "Epoch: 454\n",
      "Loss: \n",
      "Training: 1.8963498810141348 Test: 0.29667499381095447\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8550061232462647 Test: 0.3195074431676703\n",
      "Epoch: 455\n",
      "Loss: \n",
      "Training: 1.9004094670996685 Test: 0.35459094419460974\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.848319578630592 Test: 0.31886190536965364\n",
      "Epoch: 456\n",
      "Loss: \n",
      "Training: 1.9062306892269876 Test: 0.36052470655434227\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8508558637087114 Test: 0.32221821052859434\n",
      "Epoch: 457\n",
      "Loss: \n",
      "Training: 1.8478890322985855 Test: 0.2980459321192669\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8687554756217426 Test: 0.32917999927868463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 458\n",
      "Loss: \n",
      "Training: 2.036865562422599 Test: 0.3646676722490064\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8901088289561383 Test: 0.3261015737737138\n",
      "Epoch: 459\n",
      "Loss: \n",
      "Training: 1.9027774470991898 Test: 0.31949869452107943\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8868957608164343 Test: 0.33052251164401103\n",
      "Epoch: 460\n",
      "Loss: \n",
      "Training: 1.6865720162217324 Test: 0.3387842633644366\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8800047030684268 Test: 0.33021111675599635\n",
      "Epoch: 461\n",
      "Loss: \n",
      "Training: 1.8055435073348967 Test: 0.3325027414100915\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.884201175395479 Test: 0.3309645796320659\n",
      "Epoch: 462\n",
      "Loss: \n",
      "Training: 1.8168757013700896 Test: 0.31716836440722784\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8561931890848495 Test: 0.32738984918993913\n",
      "Epoch: 463\n",
      "Loss: \n",
      "Training: 1.6361912451508707 Test: 0.29218636213683014\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8778085419107686 Test: 0.327692040846875\n",
      "Epoch: 464\n",
      "Loss: \n",
      "Training: 1.7908551655692448 Test: 0.3333130797423987\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8435704549238756 Test: 0.32746446747678454\n",
      "Epoch: 465\n",
      "Loss: \n",
      "Training: 1.6133539501917082 Test: 0.3664945536032805\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8330209833793867 Test: 0.33112827606992895\n",
      "Epoch: 466\n",
      "Loss: \n",
      "Training: 1.6169840537559104 Test: 0.37605948809754525\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8043154316885903 Test: 0.33231863701079606\n",
      "Epoch: 467\n",
      "Loss: \n",
      "Training: 1.6747543839175745 Test: 0.2955035638788733\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7753907681414822 Test: 0.33387211516511633\n",
      "Epoch: 468\n",
      "Loss: \n",
      "Training: 2.0976367649815706 Test: 0.29376116379824463\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7580773033033814 Test: 0.333617878341077\n",
      "Epoch: 469\n",
      "Loss: \n",
      "Training: 1.4984762053977991 Test: 0.3227682538993455\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.764154423559279 Test: 0.3265272274960008\n",
      "Epoch: 470\n",
      "Loss: \n",
      "Training: 1.7732972475128401 Test: 0.29503727616619435\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.72372429938914 Test: 0.3268541834338274\n",
      "Epoch: 471\n",
      "Loss: \n",
      "Training: 1.863970854914291 Test: 0.3723734885768163\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7323968225182504 Test: 0.32247948471400323\n",
      "Epoch: 472\n",
      "Loss: \n",
      "Training: 1.8558654206331695 Test: 0.3076232141957372\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7382395572761902 Test: 0.3264665594306756\n",
      "Epoch: 473\n",
      "Loss: \n",
      "Training: 1.4825222719454931 Test: 0.29141792801938254\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.742138529202498 Test: 0.3255120444095266\n",
      "Epoch: 474\n",
      "Loss: \n",
      "Training: 1.816782822817308 Test: 0.30791867748541346\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.72677163188196 Test: 0.32543520099778184\n",
      "Epoch: 475\n",
      "Loss: \n",
      "Training: 1.8235941400264681 Test: 0.32959675319628473\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7293643976067663 Test: 0.3228957607720833\n",
      "Epoch: 476\n",
      "Loss: \n",
      "Training: 1.9410863449145188 Test: 0.3230162060206271\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7503884165902424 Test: 0.31920598073138373\n",
      "Epoch: 477\n",
      "Loss: \n",
      "Training: 1.8142475508457727 Test: 0.31259965226688735\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7827986457061034 Test: 0.31390165252369195\n",
      "Epoch: 478\n",
      "Loss: \n",
      "Training: 1.9272530624147566 Test: 0.3362446721605404\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.796747962398923 Test: 0.3156112613624933\n",
      "Epoch: 479\n",
      "Loss: \n",
      "Training: 1.8399847653614263 Test: 0.31493051549043294\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.779709592142242 Test: 0.3198596121987229\n",
      "Epoch: 480\n",
      "Loss: \n",
      "Training: 1.937484051536544 Test: 0.2996289361846413\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8138604481386043 Test: 0.3190758383578316\n",
      "Epoch: 481\n",
      "Loss: \n",
      "Training: 1.897856712203837 Test: 0.36992239013877143\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8302791285409747 Test: 0.3195350043596763\n",
      "Epoch: 482\n",
      "Loss: \n",
      "Training: 1.7085709772124356 Test: 0.30062656455032083\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8336677142699294 Test: 0.3192898945158718\n",
      "Epoch: 483\n",
      "Loss: \n",
      "Training: 1.9293268278380376 Test: 0.2981029195569372\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.818938269927856 Test: 0.3185902295513302\n",
      "Epoch: 484\n",
      "Loss: \n",
      "Training: 1.9336562815707816 Test: 0.31084965014276056\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8636187255171102 Test: 0.3192587287050857\n",
      "Epoch: 485\n",
      "Loss: \n",
      "Training: 1.66436477625049 Test: 0.29945754897438265\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8753060713924576 Test: 0.31955182597082044\n",
      "Epoch: 486\n",
      "Loss: \n",
      "Training: 1.9751249668886663 Test: 0.3789989514996891\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8593831350148597 Test: 0.3165379055486302\n",
      "Epoch: 487\n",
      "Loss: \n",
      "Training: 1.8699153604448437 Test: 0.3249348606923721\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8627869972122748 Test: 0.3221361800965364\n",
      "Epoch: 488\n",
      "Loss: \n",
      "Training: 2.249590632406782 Test: 0.36402453780734023\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8683537781721822 Test: 0.32336970093908485\n",
      "Epoch: 489\n",
      "Loss: \n",
      "Training: 1.7789211912242762 Test: 0.34980385925364355\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9005875351713843 Test: 0.3261476875037648\n",
      "Epoch: 490\n",
      "Loss: \n",
      "Training: 1.7924256799452007 Test: 0.3715127131987791\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8944811777576696 Test: 0.32963502188008587\n",
      "Epoch: 491\n",
      "Loss: \n",
      "Training: 1.8477752222162738 Test: 0.37445088174803925\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8799753405985353 Test: 0.33682339958149965\n",
      "Epoch: 492\n",
      "Loss: \n",
      "Training: 1.6373316073519808 Test: 0.2935254859547319\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.874967191599779 Test: 0.33727624874242645\n",
      "Epoch: 493\n",
      "Loss: \n",
      "Training: 1.979718229803165 Test: 0.2897628863271333\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8678432546137334 Test: 0.3365661408828676\n",
      "Epoch: 494\n",
      "Loss: \n",
      "Training: 2.0618107529629817 Test: 0.3095034612100715\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.872882394810246 Test: 0.3357321375598872\n",
      "Epoch: 495\n",
      "Loss: \n",
      "Training: 1.7102302522637853 Test: 0.32541073437552037\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8856978419494659 Test: 0.3355975186666183\n",
      "Epoch: 496\n",
      "Loss: \n",
      "Training: 1.8828516756000107 Test: 0.36939638249906764\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8902843895507953 Test: 0.33819283720673204\n",
      "Epoch: 497\n",
      "Loss: \n",
      "Training: 1.8089895915574308 Test: 0.3073796385900102\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8810570604219303 Test: 0.3372325803066699\n",
      "Epoch: 498\n",
      "Loss: \n",
      "Training: 2.0668425891313027 Test: 0.3400863682893045\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.874964483533189 Test: 0.3354770580964337\n",
      "Epoch: 499\n",
      "Loss: \n",
      "Training: 1.8499422921915618 Test: 0.3169243441255857\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8566896792056404 Test: 0.3330832411446301\n",
      "Epoch: 500\n",
      "Loss: \n",
      "Training: 1.9083110690039962 Test: 0.30989027213417314\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.863791789302369 Test: 0.32979528963182436\n",
      "Epoch: 501\n",
      "Loss: \n",
      "Training: 1.931061507976444 Test: 0.3655658790018171\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.875380328208249 Test: 0.32363304552536376\n",
      "Epoch: 502\n",
      "Loss: \n",
      "Training: 1.8913317223109078 Test: 0.3058645396317484\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.883708956784266 Test: 0.32274454525074153\n",
      "Epoch: 503\n",
      "Loss: \n",
      "Training: 1.995168287800378 Test: 0.37424317005168234\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9091089682801587 Test: 0.3239784506184432\n",
      "Epoch: 504\n",
      "Loss: \n",
      "Training: 1.9594405799347119 Test: 0.30397450368862183\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9106539740798802 Test: 0.3324264789908981\n",
      "Epoch: 505\n",
      "Loss: \n",
      "Training: 2.069911196315853 Test: 0.3023625113017031\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9004169567770528 Test: 0.3318735832387531\n",
      "Epoch: 506\n",
      "Loss: \n",
      "Training: 1.533418871039652 Test: 0.3157086896761223\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9363850511822598 Test: 0.32956876093137144\n",
      "Epoch: 507\n",
      "Loss: \n",
      "Training: 1.8981040593196252 Test: 0.37738516887663315\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9014417707262237 Test: 0.32419999164907687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 508\n",
      "Loss: \n",
      "Training: 1.951056791593998 Test: 0.31705873627194686\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9103532175024434 Test: 0.33120054467773913\n",
      "Epoch: 509\n",
      "Loss: \n",
      "Training: 1.814788125585158 Test: 0.3152943240241484\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8987746377487131 Test: 0.32889778147600335\n",
      "Epoch: 510\n",
      "Loss: \n",
      "Training: 1.9460181978847875 Test: 0.3680767482292611\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8952592210880723 Test: 0.32873477946585966\n",
      "Epoch: 511\n",
      "Loss: \n",
      "Training: 1.954833298849321 Test: 0.30060279174431426\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8990299339761514 Test: 0.33455342707536845\n",
      "Epoch: 512\n",
      "Loss: \n",
      "Training: 1.7447605293028239 Test: 0.3509887112139145\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9014071130634391 Test: 0.3280571183496182\n",
      "Epoch: 513\n",
      "Loss: \n",
      "Training: 1.780836222100328 Test: 0.3685437459754267\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8867499937626309 Test: 0.33256953550783475\n",
      "Epoch: 514\n",
      "Loss: \n",
      "Training: 1.712258753004765 Test: 0.3637351157921406\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8653167871926262 Test: 0.3319995931002092\n",
      "Epoch: 515\n",
      "Loss: \n",
      "Training: 1.8391103549826904 Test: 0.31662553554706524\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8405986044996312 Test: 0.3379756543105611\n",
      "Epoch: 516\n",
      "Loss: \n",
      "Training: 1.8046188920102795 Test: 0.3273757124411671\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8175185203663147 Test: 0.33940195673509727\n",
      "Epoch: 517\n",
      "Loss: \n",
      "Training: 1.944948920429467 Test: 0.30515093941118887\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8446385224633777 Test: 0.3405686590116017\n",
      "Epoch: 518\n",
      "Loss: \n",
      "Training: 1.8630645168158269 Test: 0.3188209156382217\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.849323008574362 Test: 0.3333452360650573\n",
      "Epoch: 519\n",
      "Loss: \n",
      "Training: 1.8907343066119837 Test: 0.36990342013505484\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8405237810965445 Test: 0.33352145400168476\n",
      "Epoch: 520\n",
      "Loss: \n",
      "Training: 1.6041862709660064 Test: 0.31456031696708237\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8481183991992272 Test: 0.33898236361277545\n",
      "Epoch: 521\n",
      "Loss: \n",
      "Training: 1.7333003670010176 Test: 0.3149749657784807\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8139352065073495 Test: 0.3336307204865576\n",
      "Epoch: 522\n",
      "Loss: \n",
      "Training: 1.8238838339912709 Test: 0.37493569821702033\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.791781913322519 Test: 0.33506793788997424\n",
      "Epoch: 523\n",
      "Loss: \n",
      "Training: 2.1364224474827536 Test: 0.320847940144363\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7996942437913634 Test: 0.33746263659028486\n",
      "Epoch: 524\n",
      "Loss: \n",
      "Training: 1.8707023559472757 Test: 0.36006417044202244\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8352528663296064 Test: 0.33269305600717847\n",
      "Epoch: 525\n",
      "Loss: \n",
      "Training: 2.110040272628323 Test: 0.3554250385784504\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8510972266238575 Test: 0.33232596147216664\n",
      "Epoch: 526\n",
      "Loss: \n",
      "Training: 1.7723697770152245 Test: 0.299931188648073\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8781902183884207 Test: 0.3362059117753052\n",
      "Epoch: 527\n",
      "Loss: \n",
      "Training: 1.5238088180471112 Test: 0.31266098133972003\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.874965306888915 Test: 0.3334614593959958\n",
      "Epoch: 528\n",
      "Loss: \n",
      "Training: 1.7949861516478984 Test: 0.3434720682870534\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8328512966506794 Test: 0.33421246358884893\n",
      "Epoch: 529\n",
      "Loss: \n",
      "Training: 2.1077085423557946 Test: 0.2952523275982192\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8260434601338864 Test: 0.33667757885373206\n",
      "Epoch: 530\n",
      "Loss: \n",
      "Training: 1.8620318326899523 Test: 0.32185664992512375\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8477408837082678 Test: 0.3292124696000485\n",
      "Epoch: 531\n",
      "Loss: \n",
      "Training: 1.9373247968348761 Test: 0.3136285862598194\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.873525439880662 Test: 0.32994210289585263\n",
      "Epoch: 532\n",
      "Loss: \n",
      "Training: 1.9113902109889582 Test: 0.31282100277324876\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8939278828640478 Test: 0.3298074649439865\n",
      "Epoch: 533\n",
      "Loss: \n",
      "Training: 1.9034136790066383 Test: 0.33186501351259906\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.902678520563817 Test: 0.32359599539960937\n",
      "Epoch: 534\n",
      "Loss: \n",
      "Training: 1.8730958708383239 Test: 0.2945649186460878\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.879377643716205 Test: 0.32469770273643295\n",
      "Epoch: 535\n",
      "Loss: \n",
      "Training: 1.9669441165916244 Test: 0.343020618659051\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8796169952053101 Test: 0.3181477775568395\n",
      "Epoch: 536\n",
      "Loss: \n",
      "Training: 2.03791467049774 Test: 0.36219973179771425\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.86530737960164 Test: 0.3169073355648996\n",
      "Epoch: 537\n",
      "Loss: \n",
      "Training: 1.811338308970137 Test: 0.3161686811440863\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8918618689498916 Test: 0.3231341898798637\n",
      "Epoch: 538\n",
      "Loss: \n",
      "Training: 1.831320831285886 Test: 0.3623828047303951\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9206148180421945 Test: 0.32348495986030035\n",
      "Epoch: 539\n",
      "Loss: \n",
      "Training: 1.7307013837731116 Test: 0.3191214802464906\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.924248286005993 Test: 0.32537603350463445\n",
      "Epoch: 540\n",
      "Loss: \n",
      "Training: 1.8717456720068177 Test: 0.37104952497863253\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8865475701477248 Test: 0.32776294876946155\n",
      "Epoch: 541\n",
      "Loss: \n",
      "Training: 1.5790455649364106 Test: 0.3770060525523115\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8875189540794115 Test: 0.33268223627481247\n",
      "Epoch: 542\n",
      "Loss: \n",
      "Training: 2.0288524239393326 Test: 0.3782860482245716\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8516910308895649 Test: 0.33901998290406166\n",
      "Epoch: 543\n",
      "Loss: \n",
      "Training: 1.564295294240626 Test: 0.36121499380920113\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.863437252184602 Test: 0.34556648744919394\n",
      "Epoch: 544\n",
      "Loss: \n",
      "Training: 2.0216909912024486 Test: 0.2853677963973096\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8295254137080008 Test: 0.3485014854788542\n",
      "Epoch: 545\n",
      "Loss: \n",
      "Training: 1.641324043180536 Test: 0.3169644631368881\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8443849257444136 Test: 0.34758177325397643\n",
      "Epoch: 546\n",
      "Loss: \n",
      "Training: 2.019741119527971 Test: 0.3152762668710438\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8118229184033048 Test: 0.34497615770176\n",
      "Epoch: 547\n",
      "Loss: \n",
      "Training: 1.5891883786583283 Test: 0.350875036876431\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8100055633063277 Test: 0.340283811209093\n",
      "Epoch: 548\n",
      "Loss: \n",
      "Training: 1.8666403542246972 Test: 0.330940782233896\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7877905702751467 Test: 0.3437544467823275\n",
      "Epoch: 549\n",
      "Loss: \n",
      "Training: 1.8250229740977548 Test: 0.2988773188628893\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.791322522569028 Test: 0.3406102445326776\n",
      "Epoch: 550\n",
      "Loss: \n",
      "Training: 1.7336130451073544 Test: 0.30877568961231605\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8007546816014923 Test: 0.33858582839431745\n",
      "Epoch: 551\n",
      "Loss: \n",
      "Training: 1.9740204372610093 Test: 0.3384583773378237\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.786941418911546 Test: 0.33235844485768584\n",
      "Epoch: 552\n",
      "Loss: \n",
      "Training: 1.6845307681807076 Test: 0.29906920797072134\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.826438906144006 Test: 0.32850367733623703\n",
      "Epoch: 553\n",
      "Loss: \n",
      "Training: 1.9483652115367258 Test: 0.3406116226910338\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7920067405681432 Test: 0.320581993310852\n",
      "Epoch: 554\n",
      "Loss: \n",
      "Training: 1.8601639224080715 Test: 0.32380333915808013\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8304137322977532 Test: 0.3185216561990353\n",
      "Epoch: 555\n",
      "Loss: \n",
      "Training: 1.5642194550964004 Test: 0.30640246952134853\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8142610254183154 Test: 0.32236521047511235\n",
      "Epoch: 556\n",
      "Loss: \n",
      "Training: 1.765700746215651 Test: 0.313606504505661\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8065505666099018 Test: 0.32130901111355836\n",
      "Epoch: 557\n",
      "Loss: \n",
      "Training: 2.076171199289134 Test: 0.3112061190289894\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.78114652927867 Test: 0.3211420348770201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 558\n",
      "Loss: \n",
      "Training: 2.034402577504215 Test: 0.29836313605061343\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8298448113417507 Test: 0.3171751430922759\n",
      "Epoch: 559\n",
      "Loss: \n",
      "Training: 1.9363563587865553 Test: 0.3636785769277277\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8466210336697024 Test: 0.31391737847394763\n",
      "Epoch: 560\n",
      "Loss: \n",
      "Training: 1.8864194262284746 Test: 0.31744882097349025\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8577543721385823 Test: 0.3203975042804315\n",
      "Epoch: 561\n",
      "Loss: \n",
      "Training: 1.8939403230185257 Test: 0.3773412751536405\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8730350102506947 Test: 0.32126481741654894\n",
      "Epoch: 562\n",
      "Loss: \n",
      "Training: 1.8658302649791412 Test: 0.3760769389459161\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8650269988264463 Test: 0.3251531071981306\n",
      "Epoch: 563\n",
      "Loss: \n",
      "Training: 2.042229862545565 Test: 0.35700176554189555\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8831569485062896 Test: 0.3328538802956501\n",
      "Epoch: 564\n",
      "Loss: \n",
      "Training: 2.1598683247720945 Test: 0.35245436415515696\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8925434136071733 Test: 0.3344928945807363\n",
      "Epoch: 565\n",
      "Loss: \n",
      "Training: 1.787351476460688 Test: 0.3196403897146795\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9225138538435762 Test: 0.33735799708044395\n",
      "Epoch: 566\n",
      "Loss: \n",
      "Training: 1.96232949056636 Test: 0.3737901653124254\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9448270559800043 Test: 0.33868178909977703\n",
      "Epoch: 567\n",
      "Loss: \n",
      "Training: 1.6456242656077877 Test: 0.30936379536418906\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9644899304150756 Test: 0.3447001551804535\n",
      "Epoch: 568\n",
      "Loss: \n",
      "Training: 1.8137298783175182 Test: 0.34726560862253336\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9214352370469405 Test: 0.3445159228139735\n",
      "Epoch: 569\n",
      "Loss: \n",
      "Training: 1.7474633576980843 Test: 0.31455224514079466\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.899367967128271 Test: 0.3494061700711654\n",
      "Epoch: 570\n",
      "Loss: \n",
      "Training: 1.9873380166832102 Test: 0.3331737003267702\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8804786670194236 Test: 0.34449353689247214\n",
      "Epoch: 571\n",
      "Loss: \n",
      "Training: 1.627338909466094 Test: 0.313867718989679\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.890570526064898 Test: 0.34606602482780013\n",
      "Epoch: 572\n",
      "Loss: \n",
      "Training: 1.7263169153937283 Test: 0.3063917142809688\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8639103847096543 Test: 0.339718669211404\n",
      "Epoch: 573\n",
      "Loss: \n",
      "Training: 1.7363984042959622 Test: 0.3031791985077149\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8499590497511131 Test: 0.33275014674490927\n",
      "Epoch: 574\n",
      "Loss: \n",
      "Training: 1.8503918185672985 Test: 0.32387023245775154\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8193759039261526 Test: 0.3273678900414912\n",
      "Epoch: 575\n",
      "Loss: \n",
      "Training: 1.7079270289818778 Test: 0.3760199780980722\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.788428253305673 Test: 0.32450947687175064\n",
      "Epoch: 576\n",
      "Loss: \n",
      "Training: 1.9586941537890372 Test: 0.30959694884286765\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.780485808557792 Test: 0.33014743571008986\n",
      "Epoch: 577\n",
      "Loss: \n",
      "Training: 1.8791802107992313 Test: 0.30676171095065646\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7801222748800594 Test: 0.3237281140631342\n",
      "Epoch: 578\n",
      "Loss: \n",
      "Training: 1.8206033511260822 Test: 0.29837037451161513\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8034778693992037 Test: 0.3234679056217809\n",
      "Epoch: 579\n",
      "Loss: \n",
      "Training: 1.8630124481794421 Test: 0.3673775967999753\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8041652166800606 Test: 0.31857838221068907\n",
      "Epoch: 580\n",
      "Loss: \n",
      "Training: 1.776351976048137 Test: 0.32891366935504984\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8157201257281965 Test: 0.3238609173766071\n",
      "Epoch: 581\n",
      "Loss: \n",
      "Training: 1.7836365633945628 Test: 0.334010759068924\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.794621521664689 Test: 0.32343491427943505\n",
      "Epoch: 582\n",
      "Loss: \n",
      "Training: 2.119849585528352 Test: 0.2904088844325655\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.810251287057536 Test: 0.32544921828735957\n",
      "Epoch: 583\n",
      "Loss: \n",
      "Training: 1.9223325124743322 Test: 0.3217548339783051\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8496045540709987 Test: 0.32385093530251924\n",
      "Epoch: 584\n",
      "Loss: \n",
      "Training: 1.8112973939462336 Test: 0.3150147528682627\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8681979648888352 Test: 0.32570849884957825\n",
      "Epoch: 585\n",
      "Loss: \n",
      "Training: 2.012840796061058 Test: 0.38937085199895755\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.864288522426729 Test: 0.32482295089062935\n",
      "Epoch: 586\n",
      "Loss: \n",
      "Training: 1.5290749212839838 Test: 0.33857632765508744\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.894779899134647 Test: 0.32615803828071793\n",
      "Epoch: 587\n",
      "Loss: \n",
      "Training: 1.7955009664188317 Test: 0.3243461165798528\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8518179758841413 Test: 0.3290559761619399\n",
      "Epoch: 588\n",
      "Loss: \n",
      "Training: 1.9413703691132824 Test: 0.3404459913777747\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8434500514461014 Test: 0.33081441672485956\n",
      "Epoch: 589\n",
      "Loss: \n",
      "Training: 2.039418302432634 Test: 0.30658843814140985\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8555267532448216 Test: 0.3350219784114755\n",
      "Epoch: 590\n",
      "Loss: \n",
      "Training: 1.6316790379980237 Test: 0.33218435159990334\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8731673386701406 Test: 0.328943062545619\n",
      "Epoch: 591\n",
      "Loss: \n",
      "Training: 1.9435411905678521 Test: 0.31483792824414086\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8587000448651296 Test: 0.32927013077010436\n",
      "Epoch: 592\n",
      "Loss: \n",
      "Training: 1.83628857117628 Test: 0.36709783418174663\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8746905075824583 Test: 0.327352847687626\n",
      "Epoch: 593\n",
      "Loss: \n",
      "Training: 1.961961872031694 Test: 0.2947378167447776\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.846334406147251 Test: 0.3350217426625441\n",
      "Epoch: 594\n",
      "Loss: \n",
      "Training: 1.8797165521795793 Test: 0.3228965077141924\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8502973421029876 Test: 0.3323200409391913\n",
      "Epoch: 595\n",
      "Loss: \n",
      "Training: 1.9005009913393653 Test: 0.28924637129010045\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8571392579263222 Test: 0.3331082164237843\n",
      "Epoch: 596\n",
      "Loss: \n",
      "Training: 2.051335750803805 Test: 0.3102152297783629\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8459052774541527 Test: 0.3230957683528986\n",
      "Epoch: 597\n",
      "Loss: \n",
      "Training: 1.655244476147984 Test: 0.2877165362529936\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8981313604061345 Test: 0.32025965856522615\n",
      "Epoch: 598\n",
      "Loss: \n",
      "Training: 1.968915659596404 Test: 0.2982074768324662\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8841057113790498 Test: 0.3165967005325402\n",
      "Epoch: 599\n",
      "Loss: \n",
      "Training: 1.7708202980666086 Test: 0.3230134042498299\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.886860240427362 Test: 0.3123728490780094\n",
      "Epoch: 600\n",
      "Loss: \n",
      "Training: 2.006679391743561 Test: 0.35370459670845356\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8600004399907597 Test: 0.31401534568885137\n",
      "Epoch: 601\n",
      "Loss: \n",
      "Training: 1.8650196467177367 Test: 0.3752881822634946\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.897500475365313 Test: 0.3161673701997064\n",
      "Epoch: 602\n",
      "Loss: \n",
      "Training: 2.0275735918445577 Test: 0.30964039392301185\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8896483209803019 Test: 0.3222123956016418\n",
      "Epoch: 603\n",
      "Loss: \n",
      "Training: 2.086289159905324 Test: 0.31475575531812117\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9087768230471294 Test: 0.31646665157576825\n",
      "Epoch: 604\n",
      "Loss: \n",
      "Training: 1.7704898597279781 Test: 0.3384906229216569\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9212095518344925 Test: 0.3184684454331027\n",
      "Epoch: 605\n",
      "Loss: \n",
      "Training: 1.8819000612788777 Test: 0.3500489871871949\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9102868825893324 Test: 0.3200278569538491\n",
      "Epoch: 606\n",
      "Loss: \n",
      "Training: 1.7610539558060794 Test: 0.3035485765230061\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9084267895832834 Test: 0.32610811854355853\n",
      "Epoch: 607\n",
      "Loss: \n",
      "Training: 1.9483877211152463 Test: 0.3616857112850792\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.879398610083511 Test: 0.32544145321802287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 608\n",
      "Loss: \n",
      "Training: 1.8323486472061787 Test: 0.3192144728718856\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9087129345802372 Test: 0.3328383707212314\n",
      "Epoch: 609\n",
      "Loss: \n",
      "Training: 1.8088467343669508 Test: 0.3081975701341728\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8950562333412146 Test: 0.33493907032517334\n",
      "Epoch: 610\n",
      "Loss: \n",
      "Training: 1.9272131265505152 Test: 0.2971384462824695\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8988588769712489 Test: 0.33345748691360766\n",
      "Epoch: 611\n",
      "Loss: \n",
      "Training: 1.6884571265609736 Test: 0.3391285031777247\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8909122504519444 Test: 0.32780087187100926\n",
      "Epoch: 612\n",
      "Loss: \n",
      "Training: 1.5201144069957953 Test: 0.2934567611709772\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.873255998436268 Test: 0.32418490396243227\n",
      "Epoch: 613\n",
      "Loss: \n",
      "Training: 1.7672279009077356 Test: 0.35508258677863797\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8225100799513918 Test: 0.32256654068722884\n",
      "Epoch: 614\n",
      "Loss: \n",
      "Training: 1.7349199898469734 Test: 0.33621991336571544\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7906039540516328 Test: 0.3265992238332805\n",
      "Epoch: 615\n",
      "Loss: \n",
      "Training: 1.759500118513945 Test: 0.3189620978324049\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7870469670635323 Test: 0.32637215287768634\n",
      "Epoch: 616\n",
      "Loss: \n",
      "Training: 1.6172873790836864 Test: 0.32026168751004264\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7748069727870395 Test: 0.3232634639422073\n",
      "Epoch: 617\n",
      "Loss: \n",
      "Training: 1.831697359836732 Test: 0.3363287237418845\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7604303151148002 Test: 0.32493477504091095\n",
      "Epoch: 618\n",
      "Loss: \n",
      "Training: 1.7596821409080106 Test: 0.3680377115505938\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7487612789869487 Test: 0.3223990762865915\n",
      "Epoch: 619\n",
      "Loss: \n",
      "Training: 1.9316263827889726 Test: 0.3207187545076184\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7414946283571322 Test: 0.32728140015446233\n",
      "Epoch: 620\n",
      "Loss: \n",
      "Training: 2.005881119132725 Test: 0.33113871038383424\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.753772593199334 Test: 0.3285335185918069\n",
      "Epoch: 621\n",
      "Loss: \n",
      "Training: 1.690031147127688 Test: 0.3555317051657379\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7616393924575546 Test: 0.33193354500194333\n",
      "Epoch: 622\n",
      "Loss: \n",
      "Training: 1.9273514060306272 Test: 0.3351993227980407\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7617967945142265 Test: 0.33357386520074467\n",
      "Epoch: 623\n",
      "Loss: \n",
      "Training: 1.753259451684239 Test: 0.31901326318545686\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8025204944177098 Test: 0.337748121363451\n",
      "Epoch: 624\n",
      "Loss: \n",
      "Training: 2.055004136853949 Test: 0.2808741363204858\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8011236494953604 Test: 0.3341411890041329\n",
      "Epoch: 625\n",
      "Loss: \n",
      "Training: 1.9424923672733905 Test: 0.31211959316917604\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8331320641960578 Test: 0.32860661129961\n",
      "Epoch: 626\n",
      "Loss: \n",
      "Training: 1.5512872626523877 Test: 0.34416112988621933\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8514312890720024 Test: 0.3279223608332871\n",
      "Epoch: 627\n",
      "Loss: \n",
      "Training: 1.9414878519122525 Test: 0.38985092749325706\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.844831277428872 Test: 0.33031230507090475\n",
      "Epoch: 628\n",
      "Loss: \n",
      "Training: 1.8650131140530124 Test: 0.2929948735860838\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.855810326636424 Test: 0.335664525446042\n",
      "Epoch: 629\n",
      "Loss: \n",
      "Training: 2.100041264700171 Test: 0.31621734233044796\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8663434239509242 Test: 0.32816024164959096\n",
      "Epoch: 630\n",
      "Loss: \n",
      "Training: 1.5024627007011804 Test: 0.38404999649601934\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.883184912142044 Test: 0.327710100431874\n",
      "Epoch: 631\n",
      "Loss: \n",
      "Training: 1.9007423007497106 Test: 0.35733395360692066\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8328430702988896 Test: 0.3330012290430925\n",
      "Epoch: 632\n",
      "Loss: \n",
      "Training: 2.1522889803061456 Test: 0.32893648717998947\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.853914185661092 Test: 0.33318145388721077\n",
      "Epoch: 633\n",
      "Loss: \n",
      "Training: 1.9841096035979913 Test: 0.3722920527708636\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.876407943088644 Test: 0.3325551703254056\n",
      "Epoch: 634\n",
      "Loss: \n",
      "Training: 1.8532846400126597 Test: 0.3114142904945527\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8994929582800189 Test: 0.3378830492839463\n",
      "Epoch: 635\n",
      "Loss: \n",
      "Training: 1.7183530539235947 Test: 0.28301726525611487\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8793210085958905 Test: 0.340937064701353\n",
      "Epoch: 636\n",
      "Loss: \n",
      "Training: 1.8101860364727902 Test: 0.3143825969891013\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8569070772609106 Test: 0.3380268319100469\n",
      "Epoch: 637\n",
      "Loss: \n",
      "Training: 1.7682453522286306 Test: 0.38321514363903086\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8827969546429508 Test: 0.33504897862033506\n",
      "Epoch: 638\n",
      "Loss: \n",
      "Training: 1.829301780482117 Test: 0.35195152014350617\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.865472704674589 Test: 0.3343854002349125\n",
      "Epoch: 639\n",
      "Loss: \n",
      "Training: 1.9274928056533107 Test: 0.3360451830580296\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8619015713174991 Test: 0.34028106489065474\n",
      "Epoch: 640\n",
      "Loss: \n",
      "Training: 1.789196073899327 Test: 0.31034492265387087\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8446467254128134 Test: 0.34226384896341283\n",
      "Epoch: 641\n",
      "Loss: \n",
      "Training: 1.6389707368205013 Test: 0.3301227352651866\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.873320062732628 Test: 0.33489334157919803\n",
      "Epoch: 642\n",
      "Loss: \n",
      "Training: 1.9341868497498544 Test: 0.30555215719914175\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8471429063397067 Test: 0.33217221974502464\n",
      "Epoch: 643\n",
      "Loss: \n",
      "Training: 1.8158008063617446 Test: 0.3740362599327916\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8253326932840774 Test: 0.3298337867469398\n",
      "Epoch: 644\n",
      "Loss: \n",
      "Training: 1.4817899929318932 Test: 0.32009291575423077\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.808501813560453 Test: 0.33000820746313264\n",
      "Epoch: 645\n",
      "Loss: \n",
      "Training: 1.7634061725947745 Test: 0.31722873125405077\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7713523488523766 Test: 0.33087606998910046\n",
      "Epoch: 646\n",
      "Loss: \n",
      "Training: 1.963222483982977 Test: 0.3005333578562908\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7758576607194942 Test: 0.33429721658889405\n",
      "Epoch: 647\n",
      "Loss: \n",
      "Training: 2.0389440169218522 Test: 0.3482428070244901\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.791161305470513 Test: 0.332912292675613\n",
      "Epoch: 648\n",
      "Loss: \n",
      "Training: 1.8222164506507315 Test: 0.33347215938597974\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8182311719398352 Test: 0.32941505901415896\n",
      "Epoch: 649\n",
      "Loss: \n",
      "Training: 1.825762188605227 Test: 0.3358667609745713\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8175226389566965 Test: 0.3275671229384063\n",
      "Epoch: 650\n",
      "Loss: \n",
      "Training: 2.0999340018818815 Test: 0.302287769918545\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8073495772518882 Test: 0.32754928073006045\n",
      "Epoch: 651\n",
      "Loss: \n",
      "Training: 1.9590068586988207 Test: 0.3157036682768009\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8384233700501436 Test: 0.32674356545652783\n",
      "Epoch: 652\n",
      "Loss: \n",
      "Training: 1.5678980640981381 Test: 0.2935689748192752\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8704269822379758 Test: 0.32530165875768924\n",
      "Epoch: 653\n",
      "Loss: \n",
      "Training: 1.9408772368493898 Test: 0.32998491126346546\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8337981036728042 Test: 0.32410334051970263\n",
      "Epoch: 654\n",
      "Loss: \n",
      "Training: 1.9131199896736164 Test: 0.3067916787109685\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8463057467215684 Test: 0.3196982056527701\n",
      "Epoch: 655\n",
      "Loss: \n",
      "Training: 1.9506404857551376 Test: 0.293390577673901\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8894387463957407 Test: 0.3183680819484438\n",
      "Epoch: 656\n",
      "Loss: \n",
      "Training: 1.943438380731408 Test: 0.3094917177152929\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9081621777117772 Test: 0.31598426659042883\n",
      "Epoch: 657\n",
      "Loss: \n",
      "Training: 1.7808478384430055 Test: 0.31802512211549844\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9061837673866204 Test: 0.3168801025763289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 658\n",
      "Loss: \n",
      "Training: 1.7635741740042599 Test: 0.32801239978130886\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8803741495387356 Test: 0.31385833408542985\n",
      "Epoch: 659\n",
      "Loss: \n",
      "Training: 1.8018758032658537 Test: 0.29632560142140096\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8745099218740884 Test: 0.3133123581249627\n",
      "Epoch: 660\n",
      "Loss: \n",
      "Training: 1.754359483442842 Test: 0.3379931227164524\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8721212833401508 Test: 0.30935824216964575\n",
      "Epoch: 661\n",
      "Loss: \n",
      "Training: 1.927784024075571 Test: 0.32494770845419935\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8375638314962468 Test: 0.3129287774494365\n",
      "Epoch: 662\n",
      "Loss: \n",
      "Training: 1.9320274301599318 Test: 0.3215151362567477\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8344415480339218 Test: 0.3138531814671763\n",
      "Epoch: 663\n",
      "Loss: \n",
      "Training: 1.7147017104213853 Test: 0.2937074301378625\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8708544846401014 Test: 0.31664779761092354\n",
      "Epoch: 664\n",
      "Loss: \n",
      "Training: 1.8083529717611069 Test: 0.33173543260477983\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.848236931997301 Test: 0.3130200494983633\n",
      "Epoch: 665\n",
      "Loss: \n",
      "Training: 1.9191683580202488 Test: 0.30618589340843844\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8377602302060503 Test: 0.3155144248877444\n",
      "Epoch: 666\n",
      "Loss: \n",
      "Training: 1.9707748487230525 Test: 0.3257012310337954\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8346130174325612 Test: 0.3167939564611982\n",
      "Epoch: 667\n",
      "Loss: \n",
      "Training: 1.995132804187618 Test: 0.2885501268484601\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8373466642317258 Test: 0.3184149077930484\n",
      "Epoch: 668\n",
      "Loss: \n",
      "Training: 1.7768503347710467 Test: 0.2997708988093472\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.858775160806187 Test: 0.3154674082663445\n",
      "Epoch: 669\n",
      "Loss: \n",
      "Training: 2.032814302919002 Test: 0.3444324592183345\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8601027768828657 Test: 0.31264325816914834\n",
      "Epoch: 670\n",
      "Loss: \n",
      "Training: 2.021836146316308 Test: 0.3274781301063153\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8831966268481808 Test: 0.3174539439488417\n",
      "Epoch: 671\n",
      "Loss: \n",
      "Training: 1.592472816747603 Test: 0.33371843950059826\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.909944293135527 Test: 0.316402444687828\n",
      "Epoch: 672\n",
      "Loss: \n",
      "Training: 1.9226082416974677 Test: 0.33515291152901505\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8764131724027302 Test: 0.31727951779246794\n",
      "Epoch: 673\n",
      "Loss: \n",
      "Training: 1.8623094848458321 Test: 0.35492142761158874\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.875471253556484 Test: 0.3186432953196946\n",
      "Epoch: 674\n",
      "Loss: \n",
      "Training: 2.0931072222018994 Test: 0.31613896525358304\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8902320309989284 Test: 0.32476469506706734\n",
      "Epoch: 675\n",
      "Loss: \n",
      "Training: 1.9280943055186068 Test: 0.3232159957894065\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9187074560430077 Test: 0.32320504833194763\n",
      "Epoch: 676\n",
      "Loss: \n",
      "Training: 1.9654827162829214 Test: 0.29931890858204185\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.919600050792844 Test: 0.3249080585700444\n",
      "Epoch: 677\n",
      "Loss: \n",
      "Training: 1.848516151084554 Test: 0.38206359528350337\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9190708375488306 Test: 0.32226982632486906\n",
      "Epoch: 678\n",
      "Loss: \n",
      "Training: 1.7687949933075244 Test: 0.30429591875062006\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9044091722385244 Test: 0.33162117316837336\n",
      "Epoch: 679\n",
      "Loss: \n",
      "Training: 2.0819700555669427 Test: 0.37504219255166954\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.903603638092172 Test: 0.33207367516250075\n",
      "Epoch: 680\n",
      "Loss: \n",
      "Training: 1.5676454147680183 Test: 0.32068721507339804\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.908519213356966 Test: 0.3351346484958342\n",
      "Epoch: 681\n",
      "Loss: \n",
      "Training: 1.7950720343568114 Test: 0.324938016432719\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8631001402021372 Test: 0.3344555569925424\n",
      "Epoch: 682\n",
      "Loss: \n",
      "Training: 2.014483683884944 Test: 0.3266198334764493\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.883360061963058 Test: 0.33357751468575453\n",
      "Epoch: 683\n",
      "Loss: \n",
      "Training: 1.7148368052680205 Test: 0.3552946416380544\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8925476061818056 Test: 0.332724206880498\n",
      "Epoch: 684\n",
      "Loss: \n",
      "Training: 1.7752652351332951 Test: 0.34344113207077553\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8778003382240243 Test: 0.33276152828314454\n",
      "Epoch: 685\n",
      "Loss: \n",
      "Training: 2.0123290907503684 Test: 0.34128550344276987\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.846016139517164 Test: 0.3354917449648638\n",
      "Epoch: 686\n",
      "Loss: \n",
      "Training: 1.8164678923781676 Test: 0.3566270292308637\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8544396180403404 Test: 0.3372986957302001\n",
      "Epoch: 687\n",
      "Loss: \n",
      "Training: 1.8056677548205804 Test: 0.34981918702675996\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8395381356498646 Test: 0.3430295077950823\n",
      "Epoch: 688\n",
      "Loss: \n",
      "Training: 2.030678728257303 Test: 0.30056612722854015\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8352532960234673 Test: 0.33980506696940793\n",
      "Epoch: 689\n",
      "Loss: \n",
      "Training: 1.9258818588571769 Test: 0.37486322638713054\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8614416695184453 Test: 0.3394320878171999\n",
      "Epoch: 690\n",
      "Loss: \n",
      "Training: 1.9068420320500947 Test: 0.3068205151484729\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8458328498474685 Test: 0.339414191200746\n",
      "Epoch: 691\n",
      "Loss: \n",
      "Training: 2.0213641862425686 Test: 0.3689979032651346\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.879752511575676 Test: 0.3380275212082535\n",
      "Epoch: 692\n",
      "Loss: \n",
      "Training: 1.810686402552308 Test: 0.36328762445981894\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9023817267642518 Test: 0.342433509891495\n",
      "Epoch: 693\n",
      "Loss: \n",
      "Training: 2.0822877259388193 Test: 0.31433736414004626\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8820019986309884 Test: 0.34610028898983203\n",
      "Epoch: 694\n",
      "Loss: \n",
      "Training: 2.010970089850742 Test: 0.3644412098624664\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9187470906980681 Test: 0.3420045612400312\n",
      "Epoch: 695\n",
      "Loss: \n",
      "Training: 2.0787171288409043 Test: 0.3067701003147847\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9423175761698128 Test: 0.34410456901920033\n",
      "Epoch: 696\n",
      "Loss: \n",
      "Training: 2.014591665381911 Test: 0.3153787141149744\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9489563799788663 Test: 0.34065302870640185\n",
      "Epoch: 697\n",
      "Loss: \n",
      "Training: 2.0875550571460226 Test: 0.36470755445279873\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.968768757279241 Test: 0.33652819719481286\n",
      "Epoch: 698\n",
      "Loss: \n",
      "Training: 1.7668180972569292 Test: 0.3218771137027006\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.996957487511785 Test: 0.3380170339374168\n",
      "Epoch: 699\n",
      "Loss: \n",
      "Training: 2.042557101914746 Test: 0.30886447655633337\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9705714244117476 Test: 0.34014813258483284\n",
      "Epoch: 700\n",
      "Loss: \n",
      "Training: 1.984901369346595 Test: 0.38001287444662196\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9822389487175047 Test: 0.3335482576017531\n",
      "Epoch: 701\n",
      "Loss: \n",
      "Training: 1.6420114355638922 Test: 0.3336192614298711\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9900448824471546 Test: 0.34086749353156803\n",
      "Epoch: 702\n",
      "Loss: \n",
      "Training: 1.6494388668680757 Test: 0.3321831358921068\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9521096073792865 Test: 0.33732962934804167\n",
      "Epoch: 703\n",
      "Loss: \n",
      "Training: 1.9382154850590352 Test: 0.3172267621773031\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9359848538108633 Test: 0.3342191804912704\n",
      "Epoch: 704\n",
      "Loss: \n",
      "Training: 2.032446687817145 Test: 0.3059285670919244\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9215776297228853 Test: 0.3345081202949961\n",
      "Epoch: 705\n",
      "Loss: \n",
      "Training: 1.9686982870438279 Test: 0.36027777017688134\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9237252895195258 Test: 0.3286568560179419\n",
      "Epoch: 706\n",
      "Loss: \n",
      "Training: 1.9925870585623278 Test: 0.33640123101000163\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9127234053398183 Test: 0.33400762300415154\n",
      "Epoch: 707\n",
      "Loss: \n",
      "Training: 2.0976672405244194 Test: 0.31238064595623044\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9105229446578598 Test: 0.3361098746936543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 708\n",
      "Loss: \n",
      "Training: 1.734720694135737 Test: 0.3231842872386137\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9115341629956994 Test: 0.33087718384399745\n",
      "Epoch: 709\n",
      "Loss: \n",
      "Training: 1.5354744816688566 Test: 0.2980671460453974\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9083244226835805 Test: 0.33100790119758877\n",
      "Epoch: 710\n",
      "Loss: \n",
      "Training: 1.7494183339528302 Test: 0.3017516147293701\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8576161606589914 Test: 0.32992816814649517\n",
      "Epoch: 711\n",
      "Loss: \n",
      "Training: 1.5955127329366046 Test: 0.3066483145775802\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.834067857119615 Test: 0.32210204217477\n",
      "Epoch: 712\n",
      "Loss: \n",
      "Training: 1.7483188610960445 Test: 0.38444040332168533\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.829417986856886 Test: 0.3194049474895409\n",
      "Epoch: 713\n",
      "Loss: \n",
      "Training: 1.9530663426170585 Test: 0.3150829996735428\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8393059862796828 Test: 0.32463067423249875\n",
      "Epoch: 714\n",
      "Loss: \n",
      "Training: 1.9637113630006235 Test: 0.3206745808439672\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8407910720354852 Test: 0.32441629798212274\n",
      "Epoch: 715\n",
      "Loss: \n",
      "Training: 1.8469417976768272 Test: 0.3342273584666218\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.833917539553833 Test: 0.325890899357327\n",
      "Epoch: 716\n",
      "Loss: \n",
      "Training: 1.9049870189853682 Test: 0.30498225936858264\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8217418906171332 Test: 0.32328585818630107\n",
      "Epoch: 717\n",
      "Loss: \n",
      "Training: 1.8592470174505626 Test: 0.331245766343742\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.812981886659437 Test: 0.32014396102215914\n",
      "Epoch: 718\n",
      "Loss: \n",
      "Training: 1.5307385110456426 Test: 0.2855891435358306\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7891398643520513 Test: 0.3220304730609103\n",
      "Epoch: 719\n",
      "Loss: \n",
      "Training: 1.7089237725699968 Test: 0.3637829657839204\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.768741646043042 Test: 0.318270958690632\n",
      "Epoch: 720\n",
      "Loss: \n",
      "Training: 1.8696034528058845 Test: 0.3088406676833951\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7860865751331558 Test: 0.32484254066448426\n",
      "Epoch: 721\n",
      "Loss: \n",
      "Training: 1.9972442510872963 Test: 0.29279747901057335\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7981050870184614 Test: 0.32555144595988683\n",
      "Epoch: 722\n",
      "Loss: \n",
      "Training: 1.695141387817711 Test: 0.35974848863334014\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8382782388335304 Test: 0.32416636240318614\n",
      "Epoch: 723\n",
      "Loss: \n",
      "Training: 1.831941194282718 Test: 0.3835672189504707\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8329604915056972 Test: 0.3216971709343516\n",
      "Epoch: 724\n",
      "Loss: \n",
      "Training: 1.7319482444648726 Test: 0.29574297419433015\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.820847976672263 Test: 0.32854559286204443\n",
      "Epoch: 725\n",
      "Loss: \n",
      "Training: 1.8007907120051094 Test: 0.29158028543036796\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7976716648186883 Test: 0.3260524321970807\n",
      "Epoch: 726\n",
      "Loss: \n",
      "Training: 2.013135153120162 Test: 0.31925502911004305\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7930565562515166 Test: 0.3217877248934553\n",
      "Epoch: 727\n",
      "Loss: \n",
      "Training: 1.9463498487218904 Test: 0.34825103824543385\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8038713696649957 Test: 0.3232150018676013\n",
      "Epoch: 728\n",
      "Loss: \n",
      "Training: 1.8305856464645367 Test: 0.30401790469939693\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8125816527921284 Test: 0.3249155290577705\n",
      "Epoch: 729\n",
      "Loss: \n",
      "Training: 2.0340882781355685 Test: 0.3137189364427507\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8425663663340177 Test: 0.32675840517412713\n",
      "Epoch: 730\n",
      "Loss: \n",
      "Training: 1.6419061143962828 Test: 0.30755316222676526\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8750828168905749 Test: 0.32175200224001016\n",
      "Epoch: 731\n",
      "Loss: \n",
      "Training: 1.7893419875123981 Test: 0.3867841664381539\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8523130830496146 Test: 0.32162325169434725\n",
      "Epoch: 732\n",
      "Loss: \n",
      "Training: 1.6777632066775943 Test: 0.3219525618859554\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.831522856692125 Test: 0.33102192043710527\n",
      "Epoch: 733\n",
      "Loss: \n",
      "Training: 1.924737631036166 Test: 0.30720289312070703\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8297850385781131 Test: 0.32724232776236684\n",
      "Epoch: 734\n",
      "Loss: \n",
      "Training: 1.7916927227178705 Test: 0.34100678494971043\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8390646822534582 Test: 0.3196058951793904\n",
      "Epoch: 735\n",
      "Loss: \n",
      "Training: 1.8902468725385144 Test: 0.3082891907354008\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8450391300787579 Test: 0.32413227625492846\n",
      "Epoch: 736\n",
      "Loss: \n",
      "Training: 1.9450276545149543 Test: 0.31249434989358693\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8539847461320984 Test: 0.3258031667854317\n",
      "Epoch: 737\n",
      "Loss: \n",
      "Training: 1.4399751275576333 Test: 0.29532440470014565\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8471739962715776 Test: 0.3251270988637861\n",
      "Epoch: 738\n",
      "Loss: \n",
      "Training: 1.806874776086937 Test: 0.33113140388957785\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.796536524155152 Test: 0.3198344355092573\n",
      "Epoch: 739\n",
      "Loss: \n",
      "Training: 1.6768832550203416 Test: 0.3070399900805255\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7941654371173918 Test: 0.32254578542827544\n",
      "Epoch: 740\n",
      "Loss: \n",
      "Training: 2.0073964392227106 Test: 0.36529391077630474\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.758444934805869 Test: 0.3218778907920529\n",
      "Epoch: 741\n",
      "Loss: \n",
      "Training: 1.9129122060811943 Test: 0.3786011359769996\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7949939672885118 Test: 0.3276519656470069\n",
      "Epoch: 742\n",
      "Loss: \n",
      "Training: 2.087487123769758 Test: 0.3545924679810887\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8073509891453916 Test: 0.3268336626008915\n",
      "Epoch: 743\n",
      "Loss: \n",
      "Training: 1.8075088341295853 Test: 0.3337884584281289\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8483233808546082 Test: 0.3300976532104048\n",
      "Epoch: 744\n",
      "Loss: \n",
      "Training: 1.6504948008776061 Test: 0.30796517385214367\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8366005011639501 Test: 0.33275620974114695\n",
      "Epoch: 745\n",
      "Loss: \n",
      "Training: 1.9667005451224329 Test: 0.3302463989702288\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8224807089799235 Test: 0.32945204863139027\n",
      "Epoch: 746\n",
      "Loss: \n",
      "Training: 1.707446252605211 Test: 0.3242165037176868\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8301260762383156 Test: 0.331647769454873\n",
      "Epoch: 747\n",
      "Loss: \n",
      "Training: 1.9576312672193816 Test: 0.338714320989273\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8063679360473412 Test: 0.33281998483728303\n",
      "Epoch: 748\n",
      "Loss: \n",
      "Training: 1.8977792205276194 Test: 0.30961183450879126\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8581335500135157 Test: 0.33715897646619575\n",
      "Epoch: 749\n",
      "Loss: \n",
      "Training: 1.658934119303683 Test: 0.30530607089930545\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8672239944575844 Test: 0.33500701952811707\n",
      "Epoch: 750\n",
      "Loss: \n",
      "Training: 1.7545281961922599 Test: 0.2969941163949535\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8654290808859184 Test: 0.3348336276099951\n",
      "Epoch: 751\n",
      "Loss: \n",
      "Training: 1.5839029419694703 Test: 0.31345610012803776\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8401422565828731 Test: 0.32800364817185995\n",
      "Epoch: 752\n",
      "Loss: \n",
      "Training: 1.9592020749510672 Test: 0.3689985098298735\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.807241330171701 Test: 0.32148914458696376\n",
      "Epoch: 753\n",
      "Loss: \n",
      "Training: 1.8102990151786267 Test: 0.31887886599521703\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7944128252898317 Test: 0.3229297487718422\n",
      "Epoch: 754\n",
      "Loss: \n",
      "Training: 1.7707207766014548 Test: 0.35372583833759286\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7946918433947356 Test: 0.32143878952855104\n",
      "Epoch: 755\n",
      "Loss: \n",
      "Training: 2.1552461293808736 Test: 0.316226322634736\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8067144409671205 Test: 0.326014855977096\n",
      "Epoch: 756\n",
      "Loss: \n",
      "Training: 1.7861441449269504 Test: 0.33021764055578284\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8255689993929647 Test: 0.32461284834354676\n",
      "Epoch: 757\n",
      "Loss: \n",
      "Training: 1.845759334843838 Test: 0.29405667876879626\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8334387886251384 Test: 0.3252129620273564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 758\n",
      "Loss: \n",
      "Training: 1.8177060616454492 Test: 0.31272756353927145\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8222515953875842 Test: 0.32074719780530864\n",
      "Epoch: 759\n",
      "Loss: \n",
      "Training: 1.8767298320273185 Test: 0.36524940540086337\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8142442794993674 Test: 0.32105877070835664\n",
      "Epoch: 760\n",
      "Loss: \n",
      "Training: 1.697344546601736 Test: 0.3561362142643111\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8360238507717308 Test: 0.3270531041585124\n",
      "Epoch: 761\n",
      "Loss: \n",
      "Training: 1.9986726866797253 Test: 0.33766515480644715\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8303054858126786 Test: 0.3329673139454482\n",
      "Epoch: 762\n",
      "Loss: \n",
      "Training: 1.5563052423413868 Test: 0.34011066567270726\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.871782460283704 Test: 0.33538821941328917\n",
      "Epoch: 763\n",
      "Loss: \n",
      "Training: 1.7037568777330347 Test: 0.3402611588094686\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.831492777022736 Test: 0.3324994349975725\n",
      "Epoch: 764\n",
      "Loss: \n",
      "Training: 1.558542186787459 Test: 0.34288729501927817\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.820838563278177 Test: 0.3346376642789977\n",
      "Epoch: 765\n",
      "Loss: \n",
      "Training: 1.7970511080481752 Test: 0.29296862233092313\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7996207042967771 Test: 0.3335538099471662\n",
      "Epoch: 766\n",
      "Loss: \n",
      "Training: 2.0314222687427015 Test: 0.36242870133355837\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7638012021635074 Test: 0.33122803991678496\n",
      "Epoch: 767\n",
      "Loss: \n",
      "Training: 1.9668328296166626 Test: 0.3100806788336411\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.788329014545082 Test: 0.3344491459945625\n",
      "Epoch: 768\n",
      "Loss: \n",
      "Training: 1.7928954212017343 Test: 0.3129008464265763\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8004363640223648 Test: 0.336051546001047\n",
      "Epoch: 769\n",
      "Loss: \n",
      "Training: 1.7868473117843813 Test: 0.3048872032418966\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7979552999779933 Test: 0.3360688742897774\n",
      "Epoch: 770\n",
      "Loss: \n",
      "Training: 2.075638508269283 Test: 0.32572097880096557\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7889670479536997 Test: 0.3300326540738808\n",
      "Epoch: 771\n",
      "Loss: \n",
      "Training: 1.7939037899921084 Test: 0.33038340679008726\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8267964441204545 Test: 0.32699113052754625\n",
      "Epoch: 772\n",
      "Loss: \n",
      "Training: 1.7434307694766764 Test: 0.3712701098074231\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8063195544516926 Test: 0.32626295572591024\n",
      "Epoch: 773\n",
      "Loss: \n",
      "Training: 1.786243634546634 Test: 0.34077455534124995\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8250321071652216 Test: 0.3293789001393818\n",
      "Epoch: 774\n",
      "Loss: \n",
      "Training: 1.7771862080228147 Test: 0.32927249430948996\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8332807828465814 Test: 0.32943023979256\n",
      "Epoch: 775\n",
      "Loss: \n",
      "Training: 1.8630597041734844 Test: 0.30640905640728133\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.855145184970117 Test: 0.3280687597215811\n",
      "Epoch: 776\n",
      "Loss: \n",
      "Training: 1.8790096739986861 Test: 0.3161741039926456\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8617460445826481 Test: 0.32941280312921695\n",
      "Epoch: 777\n",
      "Loss: \n",
      "Training: 1.8152020229912516 Test: 0.3205919130768678\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8465047851082468 Test: 0.3247873433951257\n",
      "Epoch: 778\n",
      "Loss: \n",
      "Training: 1.9302959522857095 Test: 0.33399948032702403\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8313417044457054 Test: 0.3258384668194484\n",
      "Epoch: 779\n",
      "Loss: \n",
      "Training: 1.736989394491825 Test: 0.3070937534011496\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.845081757554103 Test: 0.3279483302094931\n",
      "Epoch: 780\n",
      "Loss: \n",
      "Training: 1.8523235375272575 Test: 0.3202399709418054\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.840095965824847 Test: 0.32816898522541843\n",
      "Epoch: 781\n",
      "Loss: \n",
      "Training: 1.7919406687673476 Test: 0.32441410908585055\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8177644687506447 Test: 0.32762088443950244\n",
      "Epoch: 782\n",
      "Loss: \n",
      "Training: 1.9772038280497477 Test: 0.3104496787051855\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.817568156628169 Test: 0.32702395466907874\n",
      "Epoch: 783\n",
      "Loss: \n",
      "Training: 1.8822019613892313 Test: 0.32548937421818125\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8409454624854757 Test: 0.320941911558855\n",
      "Epoch: 784\n",
      "Loss: \n",
      "Training: 1.8596403912985662 Test: 0.3278451599830191\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8505412951697355 Test: 0.31941339344654807\n",
      "Epoch: 785\n",
      "Loss: \n",
      "Training: 2.0153483831796244 Test: 0.3212806161218763\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8587867134973106 Test: 0.31927066001390103\n",
      "Epoch: 786\n",
      "Loss: \n",
      "Training: 1.8340558438973873 Test: 0.3712543943686357\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8740155813979247 Test: 0.32075781598536046\n",
      "Epoch: 787\n",
      "Loss: \n",
      "Training: 1.9194137347264049 Test: 0.3174679004824264\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8695201983877952 Test: 0.3262658450229596\n",
      "Epoch: 788\n",
      "Loss: \n",
      "Training: 1.6391061428104459 Test: 0.3628254874850901\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.87994136956131 Test: 0.32595344376351537\n",
      "Epoch: 789\n",
      "Loss: \n",
      "Training: 1.8293731213205275 Test: 0.37697607587920334\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8508223886137838 Test: 0.328836044479322\n",
      "Epoch: 790\n",
      "Loss: \n",
      "Training: 1.787858244117906 Test: 0.32687055445293056\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8600607612966542 Test: 0.3358242767271274\n",
      "Epoch: 791\n",
      "Loss: \n",
      "Training: 1.8583227279796934 Test: 0.36442311226796176\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.853614231955719 Test: 0.3364873350782399\n",
      "Epoch: 792\n",
      "Loss: \n",
      "Training: 1.5409464066303826 Test: 0.30671401547111354\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8602524378769534 Test: 0.340488235396451\n",
      "Epoch: 793\n",
      "Loss: \n",
      "Training: 1.6098167683094877 Test: 0.32068377115464347\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8166266957350168 Test: 0.3401146690730438\n",
      "Epoch: 794\n",
      "Loss: \n",
      "Training: 1.7689193861614219 Test: 0.3323715259163575\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.789388176427043 Test: 0.33963410876669003\n",
      "Epoch: 795\n",
      "Loss: \n",
      "Training: 1.897864124047642 Test: 0.3604216816147722\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7803160759133285 Test: 0.3400867453600239\n",
      "Epoch: 796\n",
      "Loss: \n",
      "Training: 2.0291683613261577 Test: 0.3613955278431852\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.76856765000013 Test: 0.34400085190931345\n",
      "Epoch: 797\n",
      "Loss: \n",
      "Training: 1.9363511448583566 Test: 0.31483496871572186\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7880789017430068 Test: 0.3430149652567684\n",
      "Epoch: 798\n",
      "Loss: \n",
      "Training: 1.9175664861156234 Test: 0.3720194681068116\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.789772642756202 Test: 0.34275167208009794\n",
      "Epoch: 799\n",
      "Loss: \n",
      "Training: 1.6357406344585614 Test: 0.32251071417942295\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.81761867708672 Test: 0.34367107014227005\n",
      "Epoch: 800\n",
      "Loss: \n",
      "Training: 2.048236957912642 Test: 0.31780120208272006\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7982554284005232 Test: 0.338224533972292\n",
      "Epoch: 801\n",
      "Loss: \n",
      "Training: 1.7499993419496194 Test: 0.33941739867765064\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8242932997799968 Test: 0.337317598735271\n",
      "Epoch: 802\n",
      "Loss: \n",
      "Training: 1.9182999104558323 Test: 0.3889000222232541\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8134609611769892 Test: 0.3348170273762399\n",
      "Epoch: 803\n",
      "Loss: \n",
      "Training: 2.2179325394462017 Test: 0.34260047194039706\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8511963115595342 Test: 0.34303562805145393\n",
      "Epoch: 804\n",
      "Loss: \n",
      "Training: 1.7932502387380365 Test: 0.38095501890686556\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.912007888673206 Test: 0.3452272981300293\n",
      "Epoch: 805\n",
      "Loss: \n",
      "Training: 2.107140197709768 Test: 0.3190049270056372\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.914440973930867 Test: 0.3500856474290801\n",
      "Epoch: 806\n",
      "Loss: \n",
      "Training: 1.92142865281205 Test: 0.3758782681339294\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9353685812970798 Test: 0.3459439719681666\n",
      "Epoch: 807\n",
      "Loss: \n",
      "Training: 1.9473773181052974 Test: 0.32403082301940306\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9245946104456693 Test: 0.34739224599724106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 808\n",
      "Loss: \n",
      "Training: 1.8340927395290396 Test: 0.318473955902781\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9256972277703632 Test: 0.34831183142760913\n",
      "Epoch: 809\n",
      "Loss: \n",
      "Training: 1.6308370048542975 Test: 0.27581348621281987\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9173498531117048 Test: 0.3429572802072061\n",
      "Epoch: 810\n",
      "Loss: \n",
      "Training: 1.6747085034032738 Test: 0.33738903738877035\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9168594901512783 Test: 0.3382875574105458\n",
      "Epoch: 811\n",
      "Loss: \n",
      "Training: 1.8985313903654955 Test: 0.32552347366545703\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8795066447003415 Test: 0.34024634094115075\n",
      "Epoch: 812\n",
      "Loss: \n",
      "Training: 1.6263257511110512 Test: 0.3686299823746156\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8943598495419292 Test: 0.3388569484399314\n",
      "Epoch: 813\n",
      "Loss: \n",
      "Training: 1.4783335621197768 Test: 0.3074267551064832\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8651624336074513 Test: 0.33682994445506764\n",
      "Epoch: 814\n",
      "Loss: \n",
      "Training: 1.9616275148519517 Test: 0.3131937341497667\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7912025358748085 Test: 0.33331257277167625\n",
      "Epoch: 815\n",
      "Loss: \n",
      "Training: 1.8003712166579564 Test: 0.33803553047380513\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8080402634862 Test: 0.32653644429596634\n",
      "Epoch: 816\n",
      "Loss: \n",
      "Training: 1.6482450130779256 Test: 0.3228580096720954\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.777363365381019 Test: 0.32843950464278315\n",
      "Epoch: 817\n",
      "Loss: \n",
      "Training: 1.7764771661522718 Test: 0.29890292762627385\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7500450014076065 Test: 0.3231374787965997\n",
      "Epoch: 818\n",
      "Loss: \n",
      "Training: 1.9539869241091525 Test: 0.37493763718005585\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7329549862123037 Test: 0.32062468925728677\n",
      "Epoch: 819\n",
      "Loss: \n",
      "Training: 1.8462572429864383 Test: 0.30900699404396514\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7449444046703153 Test: 0.3262710573850143\n",
      "Epoch: 820\n",
      "Loss: \n",
      "Training: 1.9713085178477263 Test: 0.3054152045986896\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7664864284835293 Test: 0.32959040816812885\n",
      "Epoch: 821\n",
      "Loss: \n",
      "Training: 1.8901515634545616 Test: 0.3108323016116896\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7961464299279748 Test: 0.3263930248891208\n",
      "Epoch: 822\n",
      "Loss: \n",
      "Training: 1.8095949827821514 Test: 0.27732333700213835\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7953084472368812 Test: 0.32492390768374396\n",
      "Epoch: 823\n",
      "Loss: \n",
      "Training: 2.0215550245427867 Test: 0.36175474514294664\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8136353704039911 Test: 0.3157932431464963\n",
      "Epoch: 824\n",
      "Loss: \n",
      "Training: 1.7526645852921905 Test: 0.2930856714558406\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8679575166462918 Test: 0.3212260421501426\n",
      "Epoch: 825\n",
      "Loss: \n",
      "Training: 1.8109964627814326 Test: 0.3164453862421695\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8470612236903161 Test: 0.31921523588075\n",
      "Epoch: 826\n",
      "Loss: \n",
      "Training: 1.541635150837198 Test: 0.30304561539325103\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8481237483026636 Test: 0.3170562214575865\n",
      "Epoch: 827\n",
      "Loss: \n",
      "Training: 1.6908155329431458 Test: 0.30466565754742786\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8374627620785913 Test: 0.3150749820297021\n",
      "Epoch: 828\n",
      "Loss: \n",
      "Training: 1.8973459265911632 Test: 0.31326340114285883\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.828896598757678 Test: 0.31565125502181746\n",
      "Epoch: 829\n",
      "Loss: \n",
      "Training: 1.923134406772801 Test: 0.3187236274215195\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8232324990058797 Test: 0.30948383141809777\n",
      "Epoch: 830\n",
      "Loss: \n",
      "Training: 1.9810797112296044 Test: 0.2956370025690812\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8309202153845157 Test: 0.3104554947558532\n",
      "Epoch: 831\n",
      "Loss: \n",
      "Training: 1.9144989898821616 Test: 0.3267638156225441\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8318973347227032 Test: 0.3094776745528923\n",
      "Epoch: 832\n",
      "Loss: \n",
      "Training: 1.8037639430894785 Test: 0.32494283339421187\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8343320773654637 Test: 0.3110708259539777\n",
      "Epoch: 833\n",
      "Loss: \n",
      "Training: 1.7548179446623362 Test: 0.2857604649245105\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8337489733961962 Test: 0.31583277559318507\n",
      "Epoch: 834\n",
      "Loss: \n",
      "Training: 2.079291231599706 Test: 0.31114581342429737\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.807075265408151 Test: 0.3082333475713416\n",
      "Epoch: 835\n",
      "Loss: \n",
      "Training: 1.9132596392718646 Test: 0.3753934542515806\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8397379300389027 Test: 0.31003936176818725\n",
      "Epoch: 836\n",
      "Loss: \n",
      "Training: 1.8798972054944068 Test: 0.29825110087665724\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8499642476879459 Test: 0.31593416856912826\n",
      "Epoch: 837\n",
      "Loss: \n",
      "Training: 1.701133158072733 Test: 0.31137968181073317\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8837904531536669 Test: 0.315454717117469\n",
      "Epoch: 838\n",
      "Loss: \n",
      "Training: 1.9696607739356085 Test: 0.33456365753274914\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8848222156666252 Test: 0.31612611954379943\n",
      "Epoch: 839\n",
      "Loss: \n",
      "Training: 2.0141963534049663 Test: 0.32421744983494605\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.89205370040107 Test: 0.3182561451827885\n",
      "Epoch: 840\n",
      "Loss: \n",
      "Training: 1.9771654268412422 Test: 0.2893205910157343\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9011598950642867 Test: 0.31880552742413115\n",
      "Epoch: 841\n",
      "Loss: \n",
      "Training: 1.6444391839147545 Test: 0.32098877667015147\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9007684666254505 Test: 0.3181738862687965\n",
      "Epoch: 842\n",
      "Loss: \n",
      "Training: 1.7515493596108662 Test: 0.32988271946885045\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8737624860287094 Test: 0.3175963823735572\n",
      "Epoch: 843\n",
      "Loss: \n",
      "Training: 1.5684511879582679 Test: 0.3414579595793127\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8685410276808483 Test: 0.318090370981021\n",
      "Epoch: 844\n",
      "Loss: \n",
      "Training: 1.8185099134937983 Test: 0.3609556756693294\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8499043520104415 Test: 0.32366012044650116\n",
      "Epoch: 845\n",
      "Loss: \n",
      "Training: 1.833451826519805 Test: 0.3801283561791489\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.823826220199851 Test: 0.32864110667100443\n",
      "Epoch: 846\n",
      "Loss: \n",
      "Training: 1.9209247123249389 Test: 0.3355057703059847\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8158454389246448 Test: 0.32911459686376127\n",
      "Epoch: 847\n",
      "Loss: \n",
      "Training: 2.021161919798953 Test: 0.3637946432624252\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.819948189607698 Test: 0.33284006380669406\n",
      "Epoch: 848\n",
      "Loss: \n",
      "Training: 1.6951185074408226 Test: 0.2970297418636963\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8519510657803202 Test: 0.3380815599518633\n",
      "Epoch: 849\n",
      "Loss: \n",
      "Training: 1.8682306888680076 Test: 0.3542909535659987\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8244968391308418 Test: 0.33432816838495794\n",
      "Epoch: 850\n",
      "Loss: \n",
      "Training: 1.9512985079413703 Test: 0.2988881641337698\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8099002726771456 Test: 0.3373355187580632\n",
      "Epoch: 851\n",
      "Loss: \n",
      "Training: 1.9481584222235673 Test: 0.34500982727052365\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8073135807871588 Test: 0.3382922760698668\n",
      "Epoch: 852\n",
      "Loss: \n",
      "Training: 1.5495657630144637 Test: 0.2957264814430821\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8376855046180398 Test: 0.340694381129904\n",
      "Epoch: 853\n",
      "Loss: \n",
      "Training: 1.6704814008944886 Test: 0.3209415744322145\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8174871449583996 Test: 0.33727875732732715\n",
      "Epoch: 854\n",
      "Loss: \n",
      "Training: 1.8350895058137684 Test: 0.2705802684493181\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8276901662520213 Test: 0.33522711881261735\n",
      "Epoch: 855\n",
      "Loss: \n",
      "Training: 1.582599434328736 Test: 0.3729819320176398\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8293481254840185 Test: 0.3261895780906162\n",
      "Epoch: 856\n",
      "Loss: \n",
      "Training: 1.747432501617655 Test: 0.32044006370722866\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8042628862649117 Test: 0.3254749356744653\n",
      "Epoch: 857\n",
      "Loss: \n",
      "Training: 1.8386388085088794 Test: 0.31443229198518374\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7869136651941833 Test: 0.32396836501458964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 858\n",
      "Loss: \n",
      "Training: 1.8194343875633974 Test: 0.3927681476320547\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7686613540651759 Test: 0.3190321298868655\n",
      "Epoch: 859\n",
      "Loss: \n",
      "Training: 2.039087071806774 Test: 0.3584881772222197\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7810929420774333 Test: 0.3286059704637014\n",
      "Epoch: 860\n",
      "Loss: \n",
      "Training: 2.0121756883827713 Test: 0.3232032330751145\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7981785803713102 Test: 0.3290256928293235\n",
      "Epoch: 861\n",
      "Loss: \n",
      "Training: 1.6547685344537517 Test: 0.3555381780842988\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8042662984154503 Test: 0.331457199723458\n",
      "Epoch: 862\n",
      "Loss: \n",
      "Training: 1.601090916143452 Test: 0.2996902380659937\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7749273096384683 Test: 0.33251003480483554\n",
      "Epoch: 863\n",
      "Loss: \n",
      "Training: 1.7941690085298259 Test: 0.29195050743008283\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7800798249513672 Test: 0.3329064104671266\n",
      "Epoch: 864\n",
      "Loss: \n",
      "Training: 1.9363616369420604 Test: 0.36454572040801025\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7924485857149008 Test: 0.33000730376691345\n",
      "Epoch: 865\n",
      "Loss: \n",
      "Training: 1.7364401411339458 Test: 0.3036697803055774\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8025757988277307 Test: 0.33940384896278264\n",
      "Epoch: 866\n",
      "Loss: \n",
      "Training: 1.7575388198783113 Test: 0.36483010544200944\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8179598695082515 Test: 0.3324726337915764\n",
      "Epoch: 867\n",
      "Loss: \n",
      "Training: 1.8191096036994714 Test: 0.30439884238160797\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8189705013343171 Test: 0.33691163796505447\n",
      "Epoch: 868\n",
      "Loss: \n",
      "Training: 1.9538811838316876 Test: 0.31118701262272347\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.817017580853376 Test: 0.33590829300469693\n",
      "Epoch: 869\n",
      "Loss: \n",
      "Training: 1.9023053971620756 Test: 0.31876963657053203\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8304622604802048 Test: 0.3277501795037638\n",
      "Epoch: 870\n",
      "Loss: \n",
      "Training: 1.731426826670907 Test: 0.3736190705824155\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.816784093015735 Test: 0.323778325438595\n",
      "Epoch: 871\n",
      "Loss: \n",
      "Training: 1.8802325891961682 Test: 0.3553074261332517\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.788709206844549 Test: 0.3288199091893252\n",
      "Epoch: 872\n",
      "Loss: \n",
      "Training: 2.0872365784682634 Test: 0.29864619628765116\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8112556123187908 Test: 0.32879683399422044\n",
      "Epoch: 873\n",
      "Loss: \n",
      "Training: 1.8183322098410926 Test: 0.3010701679901274\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.859870178551272 Test: 0.3286924298163862\n",
      "Epoch: 874\n",
      "Loss: \n",
      "Training: 1.8249493607621785 Test: 0.32637566068403984\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8622864986823984 Test: 0.32960439587239065\n",
      "Epoch: 875\n",
      "Loss: \n",
      "Training: 1.8279065933592458 Test: 0.3104260898638699\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.85114527106441 Test: 0.32578738989999356\n",
      "Epoch: 876\n",
      "Loss: \n",
      "Training: 1.9550273771367215 Test: 0.38059044364935396\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8602919162869402 Test: 0.32646302085582285\n",
      "Epoch: 877\n",
      "Loss: \n",
      "Training: 2.0662952490400492 Test: 0.374858284475304\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8800407720127812 Test: 0.3280390546765573\n",
      "Epoch: 878\n",
      "Loss: \n",
      "Training: 1.7933808595632739 Test: 0.3358000673481127\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9047593365468392 Test: 0.3350849988859269\n",
      "Epoch: 879\n",
      "Loss: \n",
      "Training: 1.8589772374705023 Test: 0.2992864486383225\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8887093041199978 Test: 0.3375463043584658\n",
      "Epoch: 880\n",
      "Loss: \n",
      "Training: 1.7644360956489538 Test: 0.3212266993151642\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8843764881508398 Test: 0.33559798556524484\n",
      "Epoch: 881\n",
      "Loss: \n",
      "Training: 1.8669128165890856 Test: 0.3608027060657854\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8876774150486448 Test: 0.3303587484385197\n",
      "Epoch: 882\n",
      "Loss: \n",
      "Training: 1.8731025498427503 Test: 0.32653498918424156\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8863454377879365 Test: 0.3309082764317731\n",
      "Epoch: 883\n",
      "Loss: \n",
      "Training: 1.6529644506584336 Test: 0.28572696532797237\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8649320349253855 Test: 0.3336971557214322\n",
      "Epoch: 884\n",
      "Loss: \n",
      "Training: 1.8785688624816481 Test: 0.299494061349846\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8483952590071193 Test: 0.3321628354552167\n",
      "Epoch: 885\n",
      "Loss: \n",
      "Training: 1.726488777482024 Test: 0.32540902140197414\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8537572091790662 Test: 0.3294746755217973\n",
      "Epoch: 886\n",
      "Loss: \n",
      "Training: 1.7655373971883825 Test: 0.36557480664538505\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8436154275913441 Test: 0.33097296867560766\n",
      "Epoch: 887\n",
      "Loss: \n",
      "Training: 1.9854842807787783 Test: 0.31823900610629663\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8246664295965105 Test: 0.3294714049752109\n",
      "Epoch: 888\n",
      "Loss: \n",
      "Training: 1.5518271592122364 Test: 0.3061218178647326\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8165853327703831 Test: 0.3238094771383101\n",
      "Epoch: 889\n",
      "Loss: \n",
      "Training: 1.963535763154031 Test: 0.3538416771381617\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7924299627352795 Test: 0.32084165218997207\n",
      "Epoch: 890\n",
      "Loss: \n",
      "Training: 1.8702393496233454 Test: 0.3598224259159571\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.802885815303632 Test: 0.326297175039956\n",
      "Epoch: 891\n",
      "Loss: \n",
      "Training: 1.8281112224821099 Test: 0.3315365360172915\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8134661407010715 Test: 0.3301567477000352\n",
      "Epoch: 892\n",
      "Loss: \n",
      "Training: 1.6785140515736938 Test: 0.30689046049568314\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.809585981290374 Test: 0.3272301306951858\n",
      "Epoch: 893\n",
      "Loss: \n",
      "Training: 2.062260912537623 Test: 0.3036271603663952\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7901271314634681 Test: 0.32526567782633\n",
      "Epoch: 894\n",
      "Loss: \n",
      "Training: 1.9840019168768361 Test: 0.3197449157189575\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.831056777651387 Test: 0.3270556973301723\n",
      "Epoch: 895\n",
      "Loss: \n",
      "Training: 2.1816645884577124 Test: 0.3553447922960033\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8416000830909058 Test: 0.32908078276708347\n",
      "Epoch: 896\n",
      "Loss: \n",
      "Training: 1.6870815805880106 Test: 0.3045188355479614\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8871176641884744 Test: 0.33207435985648637\n",
      "Epoch: 897\n",
      "Loss: \n",
      "Training: 1.8836773597356484 Test: 0.3065332096859257\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8792720825284377 Test: 0.325968762746744\n",
      "Epoch: 898\n",
      "Loss: \n",
      "Training: 1.8168120771245366 Test: 0.32645117001955143\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8690913904241249 Test: 0.3247981831047069\n",
      "Epoch: 899\n",
      "Loss: \n",
      "Training: 1.9646117911742873 Test: 0.2816408821992107\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.895589882215355 Test: 0.32683111832018874\n",
      "Epoch: 900\n",
      "Loss: \n",
      "Training: 1.753159635974151 Test: 0.2974469493430505\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8956974850173804 Test: 0.31961103882629366\n",
      "Epoch: 901\n",
      "Loss: \n",
      "Training: 1.9621647351458975 Test: 0.2911084047041403\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8839895136524607 Test: 0.313373491169003\n",
      "Epoch: 902\n",
      "Loss: \n",
      "Training: 1.725368098761327 Test: 0.319064613017466\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8973948649188397 Test: 0.30933067803768793\n",
      "Epoch: 903\n",
      "Loss: \n",
      "Training: 1.8889947114172974 Test: 0.3177811944365044\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9020802696376027 Test: 0.3105480932898662\n",
      "Epoch: 904\n",
      "Loss: \n",
      "Training: 1.869691903122403 Test: 0.3712279476306331\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8847536495255703 Test: 0.3119634966968771\n",
      "Epoch: 905\n",
      "Loss: \n",
      "Training: 1.905993654219147 Test: 0.35650355489143026\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8733226481501268 Test: 0.3171117998880447\n",
      "Epoch: 906\n",
      "Loss: \n",
      "Training: 1.9579829361240022 Test: 0.3058418870953962\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8457555547262707 Test: 0.31722767614758735\n",
      "Epoch: 907\n",
      "Loss: \n",
      "Training: 1.8754788326632181 Test: 0.31598513491249947\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8728456902798694 Test: 0.31735998130233084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 908\n",
      "Loss: \n",
      "Training: 1.9006975567053586 Test: 0.3098074924108191\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8720258375726266 Test: 0.31830517382498824\n",
      "Epoch: 909\n",
      "Loss: \n",
      "Training: 1.5473059945124776 Test: 0.3062024985062919\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8804143855307092 Test: 0.316640806064115\n",
      "Epoch: 910\n",
      "Loss: \n",
      "Training: 1.7725937307190842 Test: 0.2879227435889093\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8386838058645278 Test: 0.3190969676948231\n",
      "Epoch: 911\n",
      "Loss: \n",
      "Training: 1.5231106340055722 Test: 0.318697886196414\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8406272153390213 Test: 0.318144547119409\n",
      "Epoch: 912\n",
      "Loss: \n",
      "Training: 1.9462609610508892 Test: 0.3111654595837885\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7967218052249887 Test: 0.3209034952686364\n",
      "Epoch: 913\n",
      "Loss: \n",
      "Training: 2.0487603135652406 Test: 0.3649865950387654\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8188110914539446 Test: 0.3201135799252686\n",
      "Epoch: 914\n",
      "Loss: \n",
      "Training: 1.6928505070352593 Test: 0.2908563273535405\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8347876516687394 Test: 0.3248341199854947\n",
      "Epoch: 915\n",
      "Loss: \n",
      "Training: 1.6825108400735467 Test: 0.38464003444663514\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8171035120600247 Test: 0.3167969579577855\n",
      "Epoch: 916\n",
      "Loss: \n",
      "Training: 1.821655099514627 Test: 0.30890586486246857\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.794755230645465 Test: 0.3196106059133059\n",
      "Epoch: 917\n",
      "Loss: \n",
      "Training: 1.907584850327085 Test: 0.3532305402325836\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7811224469845275 Test: 0.3199170036900132\n",
      "Epoch: 918\n",
      "Loss: \n",
      "Training: 1.987793391881614 Test: 0.3068206131941124\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7843330487509141 Test: 0.3236415442220216\n",
      "Epoch: 919\n",
      "Loss: \n",
      "Training: 1.7561927557957533 Test: 0.32529850468204996\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.79304263226854 Test: 0.3233428563003509\n",
      "Epoch: 920\n",
      "Loss: \n",
      "Training: 2.1181807762630362 Test: 0.37644058036734385\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8139313083968673 Test: 0.32525245691792676\n",
      "Epoch: 921\n",
      "Loss: \n",
      "Training: 1.966595446758256 Test: 0.3353002398400088\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.848490012951262 Test: 0.3341042405957702\n",
      "Epoch: 922\n",
      "Loss: \n",
      "Training: 1.767551110155508 Test: 0.28468909697241124\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8928384942265306 Test: 0.33576447596012965\n",
      "Epoch: 923\n",
      "Loss: \n",
      "Training: 1.842713145211237 Test: 0.37448257141886854\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8749675091369926 Test: 0.333116839698992\n",
      "Epoch: 924\n",
      "Loss: \n",
      "Training: 1.8778698004573071 Test: 0.31696318425649395\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8543627923015926 Test: 0.33406643733700225\n",
      "Epoch: 925\n",
      "Loss: \n",
      "Training: 1.8783636686977363 Test: 0.3214193268258926\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8728647216437975 Test: 0.33667712302729763\n",
      "Epoch: 926\n",
      "Loss: \n",
      "Training: 1.9388800708929375 Test: 0.33487479773469403\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.892450004506216 Test: 0.3303550522652234\n",
      "Epoch: 927\n",
      "Loss: \n",
      "Training: 1.6597224543159446 Test: 0.3344207089217862\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9041725016440474 Test: 0.3329519455524459\n",
      "Epoch: 928\n",
      "Loss: \n",
      "Training: 1.6211433195983695 Test: 0.37053309599455125\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8793862620429327 Test: 0.3310709624213662\n",
      "Epoch: 929\n",
      "Loss: \n",
      "Training: 1.6594258928892676 Test: 0.33581110359665906\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8427212548146084 Test: 0.33744221070141006\n",
      "Epoch: 930\n",
      "Loss: \n",
      "Training: 1.890194931184496 Test: 0.2892395192799948\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.83304456852396 Test: 0.33849347059287094\n",
      "Epoch: 931\n",
      "Loss: \n",
      "Training: 1.9455601380721077 Test: 0.2929536926139348\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.810245984016106 Test: 0.3297733644841361\n",
      "Epoch: 932\n",
      "Loss: \n",
      "Training: 1.7076581457395126 Test: 0.3790865026417371\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8081424531474912 Test: 0.32553870976152866\n",
      "Epoch: 933\n",
      "Loss: \n",
      "Training: 2.1011613408962124 Test: 0.3510131027890429\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8021531567058915 Test: 0.33497845032846124\n",
      "Epoch: 934\n",
      "Loss: \n",
      "Training: 1.841764819368457 Test: 0.30706852207200735\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8279979762743888 Test: 0.3326315034654787\n",
      "Epoch: 935\n",
      "Loss: \n",
      "Training: 1.8958565186218606 Test: 0.3351504126002542\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.824387478165504 Test: 0.33164203724703006\n",
      "Epoch: 936\n",
      "Loss: \n",
      "Training: 2.0376590957731446 Test: 0.3187995476313653\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8261367631579166 Test: 0.33301514582446623\n",
      "Epoch: 937\n",
      "Loss: \n",
      "Training: 1.5646953065452267 Test: 0.2952441221042121\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.836014665645937 Test: 0.33140762081413333\n",
      "Epoch: 938\n",
      "Loss: \n",
      "Training: 1.8045208235263337 Test: 0.3246723379429515\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8265119508688652 Test: 0.3274899621323759\n",
      "Epoch: 939\n",
      "Loss: \n",
      "Training: 1.6931322499295927 Test: 0.3096657453144318\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.844849701261662 Test: 0.3229038863272159\n",
      "Epoch: 940\n",
      "Loss: \n",
      "Training: 1.793496803558333 Test: 0.30639301965405263\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8482203369656944 Test: 0.32028935049899315\n",
      "Epoch: 941\n",
      "Loss: \n",
      "Training: 1.7942655698798975 Test: 0.30326971274018205\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8385505242030782 Test: 0.32200470053639896\n",
      "Epoch: 942\n",
      "Loss: \n",
      "Training: 1.6689104506488097 Test: 0.27118324659070003\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.823421067383857 Test: 0.3230363025490237\n",
      "Epoch: 943\n",
      "Loss: \n",
      "Training: 1.983277077543188 Test: 0.3830429997108787\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8195462978747865 Test: 0.31224597694392\n",
      "Epoch: 944\n",
      "Loss: \n",
      "Training: 1.5964932085797812 Test: 0.26806220433269257\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8077578715394842 Test: 0.31544896663610356\n",
      "Epoch: 945\n",
      "Loss: \n",
      "Training: 1.7476670419395757 Test: 0.31003139309859046\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7832307104606167 Test: 0.3115483348621721\n",
      "Epoch: 946\n",
      "Loss: \n",
      "Training: 1.9304704660327057 Test: 0.30086347411770314\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7684117627923885 Test: 0.3090364329120057\n",
      "Epoch: 947\n",
      "Loss: \n",
      "Training: 2.003339525966735 Test: 0.32195822232844284\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7576928998183443 Test: 0.30724282556063953\n",
      "Epoch: 948\n",
      "Loss: \n",
      "Training: 1.7421842063933215 Test: 0.3119327324148655\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8015573217604952 Test: 0.3099142355830625\n",
      "Epoch: 949\n",
      "Loss: \n",
      "Training: 1.8628711589565872 Test: 0.338538709010559\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.795323660047194 Test: 0.308640275030254\n",
      "Epoch: 950\n",
      "Loss: \n",
      "Training: 1.8366909827530418 Test: 0.3367951320686838\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8122975509498933 Test: 0.31152757139986664\n",
      "Epoch: 951\n",
      "Loss: \n",
      "Training: 1.6464399965995897 Test: 0.29051369153259543\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8166169688693643 Test: 0.3145677826413298\n",
      "Epoch: 952\n",
      "Loss: \n",
      "Training: 1.8356599037903478 Test: 0.3204603013658025\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8018344115413334 Test: 0.3132921805205712\n",
      "Epoch: 953\n",
      "Loss: \n",
      "Training: 1.5158455673303242 Test: 0.3360363295169164\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8185093568554873 Test: 0.31821988599808143\n",
      "Epoch: 954\n",
      "Loss: \n",
      "Training: 1.9478794493446203 Test: 0.3154273390529916\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.7717662058342012 Test: 0.3135192189786852\n",
      "Epoch: 955\n",
      "Loss: \n",
      "Training: 1.9757914371885832 Test: 0.3202728540900113\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8069048299106847 Test: 0.31825573245071503\n",
      "Epoch: 956\n",
      "Loss: \n",
      "Training: 1.9946007560270727 Test: 0.2832840858831904\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8297172694355854 Test: 0.3192798785498571\n",
      "Epoch: 957\n",
      "Loss: \n",
      "Training: 1.7065237998922966 Test: 0.3104880025596994\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.836130298435022 Test: 0.31752193972640586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 958\n",
      "Loss: \n",
      "Training: 1.9010897597157141 Test: 0.31547193000951856\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8064487258275783 Test: 0.3163749177495315\n",
      "Epoch: 959\n",
      "Loss: \n",
      "Training: 1.8394415125971435 Test: 0.30839637641679923\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8223392811598178 Test: 0.31672883750899683\n",
      "Epoch: 960\n",
      "Loss: \n",
      "Training: 2.0652808395858164 Test: 0.3264148635833928\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8199963165238735 Test: 0.3137146042496209\n",
      "Epoch: 961\n",
      "Loss: \n",
      "Training: 2.0593039941331335 Test: 0.29556655399965703\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8428553022071512 Test: 0.3126765774010917\n",
      "Epoch: 962\n",
      "Loss: \n",
      "Training: 1.9159375686038416 Test: 0.323247346800502\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8841417019605058 Test: 0.3131818636477979\n",
      "Epoch: 963\n",
      "Loss: \n",
      "Training: 1.53238309944541 Test: 0.33157329736555174\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8921694684418546 Test: 0.31346056819126783\n",
      "Epoch: 964\n",
      "Loss: \n",
      "Training: 2.124499836679863 Test: 0.32153739051976327\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8938232216533635 Test: 0.31301426497613144\n",
      "Epoch: 965\n",
      "Loss: \n",
      "Training: 1.9098820451928433 Test: 0.29859851191173575\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9114852603868873 Test: 0.3136252701228086\n",
      "Epoch: 966\n",
      "Loss: \n",
      "Training: 1.745368818781204 Test: 0.2999225701394167\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9048943211873133 Test: 0.31145783590498105\n",
      "Epoch: 967\n",
      "Loss: \n",
      "Training: 1.5929848514477525 Test: 0.3295990325766731\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8799711274627267 Test: 0.3131216843306036\n",
      "Epoch: 968\n",
      "Loss: \n",
      "Training: 2.063475818419577 Test: 0.32691908882166565\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8686172326182722 Test: 0.315032787332301\n",
      "Epoch: 969\n",
      "Loss: \n",
      "Training: 1.7700541378693084 Test: 0.3202993178182657\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8848558384886585 Test: 0.31617750321351573\n",
      "Epoch: 970\n",
      "Loss: \n",
      "Training: 1.9841774304444428 Test: 0.3335663343872053\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8779171010158753 Test: 0.3173677973536624\n",
      "Epoch: 971\n",
      "Loss: \n",
      "Training: 1.8666680456946452 Test: 0.288362748850736\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8698067601017379 Test: 0.3180829444340436\n",
      "Epoch: 972\n",
      "Loss: \n",
      "Training: 1.773430770442832 Test: 0.32635791378664053\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8505431652578888 Test: 0.3173625639191515\n",
      "Epoch: 973\n",
      "Loss: \n",
      "Training: 2.1460272130905826 Test: 0.30437079910126186\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.836292485441788 Test: 0.3176736206177654\n",
      "Epoch: 974\n",
      "Loss: \n",
      "Training: 2.072296623114201 Test: 0.32449918861746635\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8976568968063052 Test: 0.31495337079133634\n",
      "Epoch: 975\n",
      "Loss: \n",
      "Training: 1.7970542198064656 Test: 0.34435224463832415\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.892436575449739 Test: 0.31524955060110665\n",
      "Epoch: 976\n",
      "Loss: \n",
      "Training: 1.8632435875381332 Test: 0.3217232400108657\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.881153792911101 Test: 0.3198249238737655\n",
      "Epoch: 977\n",
      "Loss: \n",
      "Training: 1.8200922966115118 Test: 0.33512407615695516\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8929412697867942 Test: 0.32200499086091044\n",
      "Epoch: 978\n",
      "Loss: \n",
      "Training: 1.8508342729944087 Test: 0.32627195675755805\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9156520143031703 Test: 0.3225574952189387\n",
      "Epoch: 979\n",
      "Loss: \n",
      "Training: 1.5424053352971239 Test: 0.32039270485815996\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8943878597606532 Test: 0.3224927820125279\n",
      "Epoch: 980\n",
      "Loss: \n",
      "Training: 1.8566435230628657 Test: 0.3165015357996029\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8716229795034345 Test: 0.32250212071651724\n",
      "Epoch: 981\n",
      "Loss: \n",
      "Training: 1.9505699840269806 Test: 0.3708583861680226\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8588695887652764 Test: 0.320795640857757\n",
      "Epoch: 982\n",
      "Loss: \n",
      "Training: 1.7427499772795534 Test: 0.32650428095655953\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8672597825985104 Test: 0.3290452045894857\n",
      "Epoch: 983\n",
      "Loss: \n",
      "Training: 1.812058803187572 Test: 0.3734883899698234\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8641917032821826 Test: 0.3290598413064777\n",
      "Epoch: 984\n",
      "Loss: \n",
      "Training: 1.7403095361906213 Test: 0.37880715704341855\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.830794862291882 Test: 0.33597160039333385\n",
      "Epoch: 985\n",
      "Loss: \n",
      "Training: 1.8616290427048756 Test: 0.3202204341679865\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.797596153599524 Test: 0.34140239723592897\n",
      "Epoch: 986\n",
      "Loss: \n",
      "Training: 1.937260765489707 Test: 0.37938362202244114\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8040536358893646 Test: 0.3389892161888952\n",
      "Epoch: 987\n",
      "Loss: \n",
      "Training: 1.7752819242543387 Test: 0.31228734229909794\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8114553536845222 Test: 0.3447552543900527\n",
      "Epoch: 988\n",
      "Loss: \n",
      "Training: 2.0745352032024393 Test: 0.3837754140829431\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8069743164488046 Test: 0.342471581004267\n",
      "Epoch: 989\n",
      "Loss: \n",
      "Training: 1.8485047455881722 Test: 0.3456005727916903\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8293444094696079 Test: 0.3482219267368055\n",
      "Epoch: 990\n",
      "Loss: \n",
      "Training: 1.9611350162057493 Test: 0.32635756701424146\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8599543504987124 Test: 0.3507427135301585\n",
      "Epoch: 991\n",
      "Loss: \n",
      "Training: 2.0318164720154406 Test: 0.3344112133132874\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8704034998130008 Test: 0.3517283166516224\n",
      "Epoch: 992\n",
      "Loss: \n",
      "Training: 1.9756795191806362 Test: 0.3785295483771668\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8785281486118468 Test: 0.34808359936614897\n",
      "Epoch: 993\n",
      "Loss: \n",
      "Training: 1.74537183650092 Test: 0.32712576645935443\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9018211028019554 Test: 0.35328612610820964\n",
      "Epoch: 994\n",
      "Loss: \n",
      "Training: 1.795631045604829 Test: 0.299618139347282\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.89515240613329 Test: 0.3486498637571627\n",
      "Epoch: 995\n",
      "Loss: \n",
      "Training: 1.9493373000422132 Test: 0.3462110067185871\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9006845570747106 Test: 0.34073096198754904\n",
      "Epoch: 996\n",
      "Loss: \n",
      "Training: 1.8680318278201522 Test: 0.3641561154049678\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9094553828084446 Test: 0.34333001924260914\n",
      "Epoch: 997\n",
      "Loss: \n",
      "Training: 1.8608771210227053 Test: 0.2977495156712563\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9025324890414894 Test: 0.3418072685808618\n",
      "Epoch: 998\n",
      "Loss: \n",
      "Training: 1.6415344765918998 Test: 0.33670491982239\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.9110920087183256 Test: 0.3403534859180776\n",
      "Epoch: 999\n",
      "Loss: \n",
      "Training: 1.794697253982431 Test: 0.3571281158802944\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.867791936057272 Test: 0.3356464364920223\n",
      "Epoch: 1000\n",
      "Loss: \n",
      "Training: 1.65720375599071 Test: 0.2848816224186021\n",
      "Rolling mean of Loss (Window is 10): \n",
      "Training: 1.8624111868966975 Test: 0.3367991908008828\n",
      "Encoder's best params are saved.\n",
      "Decoder's best params are saved.\n",
      "end. \n"
     ]
    }
   ],
   "source": [
    "auto_encoder.learn(observed_arr=observed_arr, target_arr=observed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "_ = plt.figure(figsize=(20, 10))\n",
    "plt.plot(auto_encoder.verificatable_result.logs_df.train_loss.values, label=\"Train errors.\")\n",
    "plt.plot(auto_encoder.verificatable_result.logs_df.test_loss.values, label=\"Test errors.\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
